{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1.train_bertffn.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "s9NOhL1iIg44"
      ],
      "mount_file_id": "1nsj2_gsDMnalm1W5kKVlE9nG_0WSrsAE",
      "authorship_tag": "ABX9TyOLikn0hAGtUdhUwgYtyMUX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/imran1386khan/Biobert-Training-Google-Colab/blob/master/1_train_bertffn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQsicmIL9qwW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "od_H-E26KA0c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow==2.0.0-alpha0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axECUPe3a3Ay",
        "colab_type": "code",
        "outputId": "59194cd4-b2f7-4e03-db23-ce81c654da03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "!cat /proc/meminfo | egrep \"MemTotal*\"\n",
        "\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MemTotal:       13333556 kB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 15948324212355340709, name: \"/device:XLA_CPU:0\"\n",
              " device_type: \"XLA_CPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 7424318645544884706\n",
              " physical_device_desc: \"device: XLA_CPU device\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_0gVNfkKGry",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import urllib.request\n",
        "\n",
        "#Link to pre-trained BERT\n",
        "urllib.request.urlretrieve('https://github.com/naver/biobert-pretrained/releases/download/v1.0-pubmed-pmc/biobert_v1.0_pubmed_pmc.tar.gz', 'BioBert.tar.gz')\n",
        "\n",
        "if not os.path.exists('BioBertFolder'):\n",
        "    os.makedirs('BioBertFolder')\n",
        "    \n",
        "import tarfile\n",
        "tar = tarfile.open(\"BioBert.tar.gz\")\n",
        "tar.extractall(path='BioBertFolder/')\n",
        "tar.close()\n",
        "\n",
        "!wget https://github.com/re-search/DocProduct/archive/v0.2.0_dev.zip\n",
        "!wget https://github.com/re-search/gpt2-estimator/archive/master.zip\n",
        "\n",
        "!unzip master.zip\n",
        "!unzip v0.2.0_dev.zip\n",
        "\n",
        "!mv DocProduct-0.2.0_dev/* /content/\n",
        "!mv gpt2-estimator-master/* /content/\n",
        "\n",
        "!wget  https://anaconda.org/pytorch/faiss-cpu/1.2.1/download/linux-64/faiss-cpu-1.2.1-py36_cuda9.0.176_1.tar.bz2\n",
        "\n",
        "!tar xvjf faiss-cpu-1.2.1-py36_cuda9.0.176_1.tar.bz2\n",
        "!cp -r lib/python3.6/site-packages/* /usr/local/lib/python3.6/dist-packages/\n",
        "!pip install mkl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5O1wMrbh624i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir '/content/data'\n",
        "!cp '/content/drive/My Drive/data/QandA_Dataset.csv' '/content/data/train_data.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whrAoZVfGpLr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# df = pd.read_csv('/content/data/train_data.csv')\n",
        "# df = df.head(50)\n",
        "# df.to_csv('/content/data/small.csv', index=False)\n",
        "# !rm '/content/data/train_data.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_68LStlCk1ly",
        "colab_type": "code",
        "outputId": "94ac01f0-f27b-44c7-f260-223d494e7f18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        }
      },
      "source": [
        "# import keras.backend as K\n",
        "import numpy as np\n",
        "import argparse\n",
        "import cv2\n",
        "import sys\n",
        "\n",
        "import tensorflow as tf\n",
        "# tf.compat.v1.enable_eager_execution()\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.models import load_model\n",
        "import argparse\n",
        "import os\n",
        "import requests\n",
        "\n",
        "from docproduct.dataset import create_dataset_for_bert\n",
        "from docproduct.models import MedicalQAModelwithBert\n",
        "from docproduct.loss import qa_pair_loss, qa_pair_cross_entropy_loss\n",
        "from docproduct.tokenization import FullTokenizer\n",
        "from docproduct.metrics import qa_pair_batch_accuracy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "Failed to load GPU Faiss: No module named 'faiss.swigfaiss_gpu'\n",
            "Faiss falling back to CPU-only.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4aWwEQ_N-el",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir '/content/drive/My Drive/DocAssistTraining'\n",
        "!mkdir '/content/drive/My Drive/DocAssistTraining/bertffn_crossentropy'\n",
        "!mkdir '/content/drive/My Drive/DocAssistTraining/bertffn_crossentropy/bertffn'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2ZZjjhEOOVj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is used for Saving only one chkpoint and overriding it every epochs\n",
        "checkpoint_path = '/content/models/bertffn.ckpt'\n",
        "# checkpoint_path = \"/content/drive/My Drive/DocAssistTraining/bertffn_crossentropy/bertffn/bertffn.ckpt\"\n",
        "# checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Include the epoch in the file name (uses `str.format`). This can be used to save multiple chk points\n",
        "# checkpoint_path = \"/content/models/bertffn_crossentropy/bertffn/cp-{epoch:04d}.ckpt\"\n",
        "# checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "model_path='/content/drive/My Drive/DocAssistTraining/bertffn_crossentropy/bertffn/final.ckpt'\n",
        "data_path='data/'\n",
        "num_epochs=2\n",
        "num_gpu=1\n",
        "batch_size=10\n",
        "learning_rate=2e-5\n",
        "validation_split=0.2\n",
        "loss='categorical_crossentropy'\n",
        "pretrained_path='/content/BioBertFolder/biobert_v1.0_pubmed_pmc/'\n",
        "max_seq_len=512\n",
        "\n",
        "K.set_floatx('float32')\n",
        "\n",
        "if loss == 'categorical_crossentropy':\n",
        "    loss_fn = qa_pair_cross_entropy_loss\n",
        "else:\n",
        "    loss_fn = qa_pair_loss\n",
        "\n",
        "tokenizer = FullTokenizer(os.path.join(pretrained_path, 'vocab.txt'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yt12q6nblNWn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d = create_dataset_for_bert(\n",
        "    data_path, tokenizer=tokenizer, batch_size=batch_size,\n",
        "    shuffle_buffer=500000, dynamic_padding=True, max_seq_length=max_seq_len)\n",
        "\n",
        "eval_d = create_dataset_for_bert(\n",
        "    data_path, tokenizer=tokenizer, batch_size=batch_size,\n",
        "    mode='eval', dynamic_padding=True, max_seq_length=max_seq_len,\n",
        "    bucket_batch_sizes=[64, 64, 64])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_0R1StkckPD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "85495c55-f015-43fa-d7cf-e4aabbe2005f"
      },
      "source": [
        "for ele in d:\n",
        "  print(ele)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "({'a_input_ids': <tf.Tensor: id=231817, shape=(16, 88), dtype=int64, numpy=\n",
            "array([[ 101, 1267, 1240, ...,    0,    0,    0],\n",
            "       [ 101, 1930,  119, ...,    0,    0,    0],\n",
            "       [ 101, 1321, 1123, ...,    0,    0,    0],\n",
            "       ...,\n",
            "       [ 101, 1242, 1535, ...,    0,    0,    0],\n",
            "       [ 101, 1294, 1612, ...,    0,    0,    0],\n",
            "       [ 101, 2812, 4455, ...,    0,    0,    0]])>, 'a_input_masks': <tf.Tensor: id=231818, shape=(16, 88), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'a_segment_ids': <tf.Tensor: id=231819, shape=(16, 88), dtype=int64, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]])>, 'q_input_ids': <tf.Tensor: id=231821, shape=(16, 50), dtype=int64, numpy=\n",
            "array([[  101,  1139,   191, 27547,  1605, 15483,   170,  1374,  1552,\n",
            "         1170,  2673,   119,  1175,  1110,  1185, 22832, 12398,  1105,\n",
            "         1185, 21430,   119,  1198,   170,  4968,  8920,  2296,  1211,\n",
            "         1104,  1103,  1159,   119,   136,   102,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0],\n",
            "       [  101,  1139,  1679,  2042,  1818,  1110,   170,  8992,  1105,\n",
            "        14947,  1165,   185,  3051,   119,  2999, 12398,   117,  1185,\n",
            "         1122,  7520,   117,  1198,   178, 14791, 24558,   119,  1110,\n",
            "         1142,  1198,  1121,  5902,  2673, 19863,  6194,  1137,  1110,\n",
            "         1175,   170,  2657,  2486,   136,   102,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0],\n",
            "       [  101,  1139,   127,  1214,  1385,  1797, 19073,  1116,  1164,\n",
            "          191, 27547,  7050,  1122,  7520,  1105, 19614,   119,  1195,\n",
            "         1274,   112,   189,  1202, 13877, 26066,   119,  1184,  1169,\n",
            "          178,  1202,  1111,  1123,   136,   102,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0],\n",
            "       [  101,  1139,  1268,  1334,  1104,  1139,   191, 27547,  7050,\n",
            "         1298,  1110, 13930,   178,  1238,   112,   189,  1138,  2673,\n",
            "         1133,  1122, 15483,  1105, 14947,  1165,   178,   185,  3051,\n",
            "         1105,  1122,  7486,  1541,  6118,   178,  1138,  1185,   191,\n",
            "        27547,  7050, 24024,  1909,  1149,  1133,  1184,  1169,  1129,\n",
            "         1103,  2463,   136,   178,   102],\n",
            "       [  101,  2489,  1107,  1139,   191, 27547,  1605,  1165,   178,\n",
            "        14182,   117,  1105,   170, 13930,  3472,   136,   102,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0],\n",
            "       [  101,  1141,  1334,  1104,  1139,  8074,  1465,  1110, 13930,\n",
            "          111, 15483,  1165,   178,  2828,  1122,  1137, 14182,  1170,\n",
            "          185,  3051,  1158,   119,  1185, 21430,  1137, 12398,   136,\n",
            "          102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0],\n",
            "       [  101,  1139,   191, 27547,  1605, 15483,  6118,   183, 14947,\n",
            "         1165,  1122,  7520,  1184,  1180,   171,  1103,  2463,   136,\n",
            "          102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0],\n",
            "       [  101,  4268,  1494,   106,  1400,  5182,  1111,   191, 27547,\n",
            "         2605,  6620,  1133,  1253,  2296,  8006,  1104, 15939,   120,\n",
            "        18767,  2489,  1107,   191, 27547,  1605,   119,  1139,   191,\n",
            "        27547,  1605,  1145,  5115, 13930,   119,  1184,  1110,  1280,\n",
            "         1113,   136,   102,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0],\n",
            "       [  101,   185,  1883, 15901,  2489,  1105, 13930,  8074,  1465,\n",
            "         1114,   170,  1376,  1122,  1732,  1208,  1133,   178,  1125,\n",
            "         1126, 12030,  1593,   123,  2277,  2403,  1431,   178,  1301,\n",
            "         1106, 14044,  1137,   176,  5730,   136,   102,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0],\n",
            "       [  101,  1139,   191, 27547,  1605,  1110,  1122,  8992,  1178,\n",
            "         2121,  1463,   117,  1122,   112,   188,  1151,  1280,  1113,\n",
            "         1111,  1593,   170,  1989,   117,  1185,  4968,  1137,  2489,\n",
            "         1198,  8362,  8178, 13199,  8637,   119,  1184,  1202,   178,\n",
            "         1202,   136,   102,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0],\n",
            "       [  101,  1139,   191, 27547,  1605,  1110,  1122,  7520,  1572,\n",
            "          120,   128,  8006,  1511,   117, 13930,  2089,   117,  4968,\n",
            "         1165,   190,  9324,  1916,  1105,  4836,  1122,  7520,   119,\n",
            "         2495, 26405,  6354,  1674,  1136,  1250,   119,  1122, 15483,\n",
            "         1304,  2213,   119,   136,   102,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0],\n",
            "       [  101,  3008,  1177,   178,  1138,  1151,  1515,  8006,  1104,\n",
            "          170, 25693,  8974,  1133,   178,  1631,  7688,  8886,  1126,\n",
            "         1116, 13930,  1213,  1105,  1113,   180,  1183,   191, 27547,\n",
            "         1605,   119,   119,   119,   119,  1184,  1169,  2612,  1142,\n",
            "          136,   102,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0],\n",
            "       [  101,  1139,  1126,  1361,  1122,  7520,  1105,  1139, 11030,\n",
            "         1596,  1716,  1145,  1139,  1656,   191, 27547,  1605,  1315,\n",
            "          119,   119,   119,  1165,   181,  1301,  1106,   185,  3051,\n",
            "         1110,  4968,  1105,  8920,  1139,   170,  1830,  8380, 14503,\n",
            "         1233,  1110,  8920,  1110,  1115,  1251,   188,  1204,  1181,\n",
            "         1494,   136,   102,     0,     0],\n",
            "       [  101,  1139,   191,  1403,  1110,  1122,  8992,  1105,  5115,\n",
            "        17072,   117,  1185,  2489,   117,  1185,  4968, 14516,  2316,\n",
            "         1185,  1653,  6187,  7111,  1403, 13445,  1991,  1114,  5346,\n",
            "         1181,  1447,  1105,  3093,  4395,  1169,  1122,  1129,   170,\n",
            "        25693,   136,   102,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0],\n",
            "       [  101,  1139,   127,  1214,  1385,  1797, 19073,  1116,  1164,\n",
            "          191, 27547,  7050,  1122,  7520,  1105, 19614,   119,  1195,\n",
            "         1274,   112,   189,  1202, 13877, 26066,   119,  1184,  1169,\n",
            "          178,  1202,  1111,  1123,   136,   102,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0],\n",
            "       [  101,  1139,  1910,  1110, 13992, 19380,  1105,  1131,  1163,\n",
            "         1122,   112,   188, 13930,  1205,  1147,  1131,  1500,  1143,\n",
            "         1115,  1131,  1508,   170, 27629, 24729,  1179,  1114, 26063,\n",
            "        13830,  3740,  1146,  1105,  1122, 13930,   136,   102,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0]])>, 'q_input_masks': <tf.Tensor: id=231822, shape=(16, 50), dtype=int64, numpy=\n",
            "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0]])>, 'q_segment_ids': <tf.Tensor: id=231823, shape=(16, 50), dtype=int64, numpy=\n",
            "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0]])>, 'labels': <tf.Tensor: id=231820, shape=(16,), dtype=int64, numpy=array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])>}, <tf.Tensor: id=231824, shape=(16,), dtype=int64, numpy=array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])>)\n",
            "({'a_input_ids': <tf.Tensor: id=231833, shape=(9, 38), dtype=int64, numpy=\n",
            "array([[  101,   191, 27547,  2605,  6620,   136,   191,  4654,  5086,\n",
            "         6620,   136,  1267,  1123,  3995,   119,   102,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0],\n",
            "       [  101,  2927,  1582, 27719,   112,   188,   172,  6834,  1204,\n",
            "          119,  1211,  2620,  2927,  1582, 27719,   112,   188,   176,\n",
            "         1931, 24970,   172,  6834,  1204,  1137,   170,  4832, 22371,\n",
            "         1209,  1444,  2848, 25523,  3566,  6059,   119,   102,     0,\n",
            "            0,     0],\n",
            "       [  101,  1111,   123,  2277,  1105,  1704,   177, 21431,   102,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0],\n",
            "       [  101,  1128,  1444,  1106,  1267,  1240,  3995,  1106,  1138,\n",
            "         1122,  5708, 11534,  1105,  5165,   119,   102,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0],\n",
            "       [  101,  1128,  2654,  1515,  8974,  1104,   191, 27547,  1605,\n",
            "         1105,  1168,  1226,  1104,  1404,  1112, 13930,  3472,   119,\n",
            "         1618, 27231,  1240,  3995,   117,  1128,  1336,  1444,  1199,\n",
            "         4449,  1105,  1243,  3252,  1408,  1112,  1770,  1112,  1936,\n",
            "          119,   102],\n",
            "       [  101,   191, 27547,  1605,  4968,   119,  3807,  1176,   170,\n",
            "          112, 25693,  8974,   112,  1137,   190,  3121,  1431,  1243,\n",
            "         6488,   119,   102,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0],\n",
            "       [  101,  1243,  1126, 12211,   119,  1122,   112,   188,  1177,\n",
            "         1662,  1106,  1587,  1443, 19924, 13766,  1103, 20085,   119,\n",
            "         1191,  1122,  2736,  2418,   117,   178,  1341,  1122,   112,\n",
            "          188,  1436,  1106,  1294,  1126,  5516,   119,   102,     0,\n",
            "            0,     0],\n",
            "       [  101,  8974,   119,  1128,  1138,  1126,  8974,   119,  1142,\n",
            "         1110,  3337,   170, 25693,  1137,  4106,  6997,  8974,   119,\n",
            "          102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0],\n",
            "       [  101,  1267,  4167, 21943,  8072,  1111,  2590,  1105,  3252,\n",
            "          119,   102,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0]])>, 'a_input_masks': <tf.Tensor: id=231834, shape=(9, 38), dtype=int64, numpy=\n",
            "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])>, 'a_segment_ids': <tf.Tensor: id=231835, shape=(9, 38), dtype=int64, numpy=\n",
            "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])>, 'q_input_ids': <tf.Tensor: id=231837, shape=(9, 46), dtype=int64, numpy=\n",
            "array([[  101,  1139,  1910,  1125,  1894,  1757,  1107,  1123,   191,\n",
            "        27547,  1605,  1105,  1105,  1122, 15483,   119,  1184,  1110,\n",
            "         2488,  1114,  1123,   136,   102,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0],\n",
            "       [  101,  1141,  1334,  1104,  6144,   191, 27547,  7050,  2280,\n",
            "         1304, 13930,   119,  1185, 12398,  1137,  2489,  2020,  1106,\n",
            "         1122,   119,   136,   102,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0],\n",
            "       [  101,  1139,   191, 27547,  1605, 17975,  1116,  1177,  2213,\n",
            "          119,  1136,  1165,   178,   190,  9324,  1566,   117,  1133,\n",
            "         1122,  1198, 17975,  1116,  1107,  1704,  1213,  1103,  2280,\n",
            "         1104,  1122,   119,  1122,  2736,  2999,   111,   178,  1138,\n",
            "         1185, 12398,   136,  3242,  1106,  1177, 12858,  1162,   136,\n",
            "          102],\n",
            "       [  101,  2489,  1229,  1515,  2673,  5115,  1176,  1103, 21504,\n",
            "         1110,  1217,  8192,  1118,  2241,  1137,  1110, 13930,   119,\n",
            "          119,  1122,  1110,  1145, 15939,  1213,   191, 27547,  1605,\n",
            "         1105, 14947,  1165,   178,   185,  3051,  1185, 12398,   136,\n",
            "          102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0],\n",
            "       [  101,  2489,  1107,  1139,   191, 27547,  1605,  1165,   178,\n",
            "        14182,   117,  1105,   170, 13930,  3472,   136,   102,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0],\n",
            "       [  101,  1139,   191, 27547,  1605, 15483,  6118,   183, 14947,\n",
            "         1165,  1122,  7520,  1184,  1180,   171,  1103,  2463,   136,\n",
            "          102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0],\n",
            "       [  101,  1139,  1286,   191, 27547,  7050,  4764,  1110, 13930,\n",
            "          119,   178,  1274,   112,   189,  1138,  2489,  1137,  1122,\n",
            "         7520,   119, 20049,  1204,  1431,   178,  1202,  1208,   136,\n",
            "          102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0],\n",
            "       [  101,  1139,   191, 27547,  1605,  1110,  1122,  7520,   111,\n",
            "         4968,  1541,  2213,   119,  4882,  1122,   112,   188,  2547,\n",
            "         1106, 21835,  1146,  1184,  1110,  1142,   136,  1184,   112,\n",
            "          188, 23831,  1113,   136,   102,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0],\n",
            "       [  101,  1139,  1676,  1144,  1126,  1122,  7520,  1268,  1807,\n",
            "         1123, 19393, 25758,   119,  1184,  1180,  1142,  1129,   136,\n",
            "          102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0]])>, 'q_input_masks': <tf.Tensor: id=231838, shape=(9, 46), dtype=int64, numpy=\n",
            "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0]])>, 'q_segment_ids': <tf.Tensor: id=231839, shape=(9, 46), dtype=int64, numpy=\n",
            "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0]])>, 'labels': <tf.Tensor: id=231836, shape=(9,), dtype=int64, numpy=array([1, 1, 1, 1, 1, 1, 1, 1, 1])>}, <tf.Tensor: id=231840, shape=(9,), dtype=int64, numpy=array([1, 1, 1, 1, 1, 1, 1, 1, 1])>)\n",
            "({'a_input_ids': <tf.Tensor: id=231849, shape=(12, 100), dtype=int64, numpy=\n",
            "array([[ 101, 1122, 3807, ...,    0,    0,    0],\n",
            "       [ 101, 1894, 4252, ...,    0,    0,    0],\n",
            "       [ 101, 2774, 1106, ...,    0,    0,    0],\n",
            "       ...,\n",
            "       [ 101, 8179,  119, ...,    0,    0,    0],\n",
            "       [ 101,  192, 1766, ...,    0,    0,    0],\n",
            "       [ 101, 1267, 1240, ...,    0,    0,    0]])>, 'a_input_masks': <tf.Tensor: id=231850, shape=(12, 100), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'a_segment_ids': <tf.Tensor: id=231851, shape=(12, 100), dtype=int64, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]])>, 'q_input_ids': <tf.Tensor: id=231853, shape=(12, 48), dtype=int64, numpy=\n",
            "array([[  101,  2489,  1107,  1139,   191, 27547,  1605,  1165,   178,\n",
            "        14182,   117,  1105,   170, 13930,  3472,   136,   102,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0],\n",
            "       [  101,  1139,  1126,  1361,  1144,  1151,  1122,  8992,  1105,\n",
            "         4968,   119,  1355,  1283,  1105,  1110,  1208,  1171,   119,\n",
            "         1185, 12398,  1104, 23609,  1116,  1137,  1892,   119,  1185,\n",
            "         2463,  1114,  7329,  2230,   119,  1145,  1894,  1213,  1298,\n",
            "          119,  1494,   136,   102,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0],\n",
            "       [  101,  1176,   126,   194,  1733,  2403,  1106,  1208,   178,\n",
            "          112,  1396,  1125,  1199,  1231, 13335, 10182, 15378,  1122,\n",
            "         7520,  1105,  1165,   178, 15844,  1199,  4968,   183,   191,\n",
            "        27547,  1605,  1185,  2489,  1309,  1105,  1309,  3535, 15939,\n",
            "         1116,   119,   119,  1110,  1122,  1123,  6633,   136,   102,\n",
            "            0,     0,     0],\n",
            "       [  101,  1141,  1334,  1104,  1139,  8074,  1465,  3137,  1110,\n",
            "        13930,   119,  1145,  1122,  8992,   119,  1175,  1110,  1185,\n",
            "        12398,  4040,  4968,  1165,   178,   190,  9324,  1566,   119,\n",
            "         1184,   112,   188,  1103,  2612,   136,   102,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0],\n",
            "       [  101,  1139,  5047,   191, 27547,  1605,  4764,  1110,  4968,\n",
            "          120, 25648,   119,  1185, 21090,   119,  1178,   170,  1376,\n",
            "         1226,  1115, 19565,  1205,  1110,  4968,   120, 25648,   119,\n",
            "         1184,   112,   188,  2488,   136,   102,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0],\n",
            "       [  101,  1139,   191, 27547,  1605,  1144,  1151,  2776, 17072,\n",
            "          120,  4968,  8710,   120,  1122,  1732,  1165,  1217,  8589,\n",
            "         1329, 15811,  7246,  2241,  1208,  1157,  8920,  1165,  1515,\n",
            "         2673,  1184,  1122,  1180,   171,   136,   102,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0],\n",
            "       [  101,  1139,  1126,  1361,  1122,  7520,  1105,  1139, 11030,\n",
            "         1596,  1716,  1145,  1139,  1656,   191, 27547,  1605,  1315,\n",
            "          119,   119,   119,  1165,   181,  1301,  1106,   185,  3051,\n",
            "         1110,  4968,  1105,  8920,  1139,   170,  1830,  8380, 14503,\n",
            "         1233,  1110,  8920,  1110,  1115,  1251,   188,  1204,  1181,\n",
            "         1494,   136,   102],\n",
            "       [  101,  1139,   191, 27547,  7050,  1298, 15483,  1165,   178,\n",
            "          185,  3051,  1105,   178,  1138,   170, 12398,  1169,  1494,\n",
            "         2482,  1149,  1725,   136,   102,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0],\n",
            "       [  101,  8920,   190,  9324,  2116,  1122, 12285,  5800,  4968,\n",
            "         1353,  1653, 21090,  1213,   191, 27547,  1605,  1185, 12398,\n",
            "         1105,  3819,  3179,   136,   102,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0],\n",
            "       [  101,  1139,   190,  8127, 20955,  2280,  1110, 13930,  1105,\n",
            "         1107,  2087,  7609,  4611,   119,  7246,  1106,  1103,  2828,\n",
            "         1133,  1185,  2489,  1165,   190,  9324,  1916,   119,   178,\n",
            "         1125,  9793,  1170,  2673,  1373,  1114,  1103, 24970,   136,\n",
            "          102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0],\n",
            "       [  101,  1139,  1910,  1110, 13992, 19380,  1105,  1131,  1163,\n",
            "         1122,   112,   188, 13930,  1205,  1147,  1131,  1500,  1143,\n",
            "         1115,  1131,  1508,   170, 27629, 24729,  1179,  1114, 26063,\n",
            "        13830,  3740,  1146,  1105,  1122, 13930,   136,   102,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0],\n",
            "       [  101,  1139,   191, 27547,  1605, 15483,  6118,   183, 14947,\n",
            "         1165,  1122,  7520,  1184,  1180,   171,  1103,  2463,   136,\n",
            "          102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0]])>, 'q_input_masks': <tf.Tensor: id=231854, shape=(12, 48), dtype=int64, numpy=\n",
            "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0]])>, 'q_segment_ids': <tf.Tensor: id=231855, shape=(12, 48), dtype=int64, numpy=\n",
            "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0]])>, 'labels': <tf.Tensor: id=231852, shape=(12,), dtype=int64, numpy=array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])>}, <tf.Tensor: id=231856, shape=(12,), dtype=int64, numpy=array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])>)\n",
            "({'a_input_ids': <tf.Tensor: id=231865, shape=(3, 108), dtype=int64, numpy=\n",
            "array([[  101,  1202,  1128,  1138,  8920,   190,  9324,  2116,   136,\n",
            "         8974,  1104,   190,  8127, 20955,  1110,  1103,  1211,  1887,\n",
            "         2612,  1104,   190,  8127,  8167, 10721,   113, 20085,  1104,\n",
            "          190,  8127, 20955,   114,   119,  4182,  1104,   190,  8127,\n",
            "        20955,  1110,  4054,   119,  1892,  1107,  1103, 19968,   117,\n",
            "         6539,   190,  9324,  2116,   117,   190,  8127, 20955,  1233,\n",
            "        12398,   117, 12089, 26310,  1107,  1103, 22106,  1132,  1675,\n",
            "          119,  3187,  5320,  1111,  4182,  1132,  1607,  1104, 28083,\n",
            "         4182,   117,  1217,  2539,   194,  1733,  1105,  2214,   117,\n",
            "         1653,  2130,   117, 13306,  2716,  1894,   117, 13930,   190,\n",
            "         8127, 20955,   119, 27231,  1240,   176, 10941, 27427,  1304,\n",
            "         1770,   119,   102,     0,     0,     0,     0,     0,     0],\n",
            "       [  101, 25693,  1137, 10548,   119,  1191,  1128,  1202,  1136,\n",
            "         6297,  1106,  2848, 14703, 12253,  1233,  6716,  2316,   117,\n",
            "         1122,  1110,  1136, 14124,  1106,  1138,   170, 19560,  8974,\n",
            "         1134,  1169, 27180,  1103,   188, 17162,  1643, 27460,  1104,\n",
            "          170, 25693,  8974,   119, 19560,   191, 27547, 27078,  1110,\n",
            "         1141,  1104,  1103,  1211,  1887,  4680,  1104,   191, 27547,\n",
            "         7050, 12398,   119,   191,  4654,  5086,  6620,  1110,  1330,\n",
            "          185, 13159, 13292, 11796,  1112,  1110,   189, 10886, 18445,\n",
            "         5813,  4863,   117, 22572,  7609, 19429,  1465,  1105,  1301,\n",
            "        13523,  1197, 13836,   119, 14837, 16565,  1216,  1112,  1123,\n",
            "         6633,  1137,   185, 11478, 12284,  1918,  7942,  1336,  1884,\n",
            "          118,  4056,  1133,  1675,  1114,  1472,  8006,   119,   102],\n",
            "       [  101,   185,  1883, 15901,  2489,   119,  2812,  1146,  1114,\n",
            "         1240,   176,  5730,  1191,  1103,   185,  1883, 15901,  2489,\n",
            "         1110, 10496,  1106,  8828,   119,  1191,  1515,  5199,   185,\n",
            "         1883, 15901,  2489,  1137,  2302,  9793,  1128,  1444,  1106,\n",
            "         1301,  1106,  1103, 14044,   119,  1128,  1169,  1202,  7250,\n",
            "        10919,  1279,  1114,  3258,  1447,  1105,   170,  6707,  2365,\n",
            "         1104, 23230,  5526,  1106,  1494,  1114,  1103,  8074,  2916,\n",
            "        20085,  1105,  1122,  7520,   119,  1294,  1612,  1128,  2415,\n",
            "         2914,  2673,  1105,  1329,  3485,  1654,  1177,  1128,  1274,\n",
            "          112,   189,  1518,  1138,  1106,  1301,  1194,  1142,  1254,\n",
            "          119,  2810,  1128,  1631,  1618,  1770,   119,   102,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0]])>, 'a_input_masks': <tf.Tensor: id=231866, shape=(3, 108), dtype=int64, numpy=\n",
            "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])>, 'a_segment_ids': <tf.Tensor: id=231867, shape=(3, 108), dtype=int64, numpy=\n",
            "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])>, 'q_input_ids': <tf.Tensor: id=231869, shape=(3, 44), dtype=int64, numpy=\n",
            "array([[  101,  1139,   190,  8127, 20955,  1233,  2280,  1110, 13930,\n",
            "         1114,   170,  1894, 15645,  1107,  2057,   119,  1185,  1122,\n",
            "         7520,   117,  4968,   119,  1185,  2673,  1107,   123,   194,\n",
            "         1733,   119,   178,   112,   182,  5073,   194,  1197,   119,\n",
            "         1385,  2130,   119,  2612,  1111,  4517,   136,   102],\n",
            "       [  101,  3008,  1177,   178,  1138,  1151,  1515,  8006,  1104,\n",
            "          170, 25693,  8974,  1133,   178,  1631,  7688,  8886,  1126,\n",
            "         1116, 13930,  1213,  1105,  1113,   180,  1183,   191, 27547,\n",
            "         1605,   119,   119,   119,   119,  1184,  1169,  2612,  1142,\n",
            "          136,   102,     0,     0,     0,     0,     0,     0],\n",
            "       [  101,   185,  1883, 15901,  2489,  1105, 13930,  8074,  1465,\n",
            "         1114,   170,  1376,  1122,  1732,  1208,  1133,   178,  1125,\n",
            "         1126, 12030,  1593,   123,  2277,  2403,  1431,   178,  1301,\n",
            "         1106, 14044,  1137,   176,  5730,   136,   102,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0]])>, 'q_input_masks': <tf.Tensor: id=231870, shape=(3, 44), dtype=int64, numpy=\n",
            "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])>, 'q_segment_ids': <tf.Tensor: id=231871, shape=(3, 44), dtype=int64, numpy=\n",
            "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])>, 'labels': <tf.Tensor: id=231868, shape=(3,), dtype=int64, numpy=array([1, 1, 1])>}, <tf.Tensor: id=231872, shape=(3,), dtype=int64, numpy=array([1, 1, 1])>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTNtB8PN4HcL",
        "colab_type": "code",
        "outputId": "733042b3-5c16-41db-b6fb-cc84a40f1782",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "medical_qa_model = MedicalQAModelwithBert(\n",
        "    config_file=os.path.join(\n",
        "        pretrained_path, 'bert_config.json'),\n",
        "    checkpoint_file=os.path.join(pretrained_path, 'biobert_model.ckpt'))\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
        "\n",
        "medical_qa_model.compile(\n",
        "    optimizer=optimizer, loss=loss_fn, metrics=[qa_pair_batch_accuracy])\n",
        "\n",
        "epochs = num_epochs\n",
        "\n",
        "callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_path, verbose=1, save_weights_only=True, save_best_only=False, period=1)\n",
        "\n",
        "medical_qa_model.fit(d, epochs=epochs, callbacks=[callback])\n",
        "\n",
        "medical_qa_model.summary()\n",
        "\n",
        "# medical_qa_model.save_weights(model_path)\n",
        "\n",
        "# Save the weights using the `checkpoint_path` format. Enable for saving multiple chk points\n",
        "# medical_qa_model.save_weights(checkpoint_path.format(epoch=0))\n",
        "\n",
        "medical_qa_model.evaluate(eval_d)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "[[[[0.230596691 -0.133663878 -0.262022525 ... 0.0568055846 -0.00428466499 0.012978144]\n",
            "  [-0.281676114 0.76460129 -0.346016884 ... 0.0748428404 0.443163812 0.802191675]\n",
            "  [-0.361676395 0.245992765 0.272511661 ... -0.248312011 0.467030674 -0.409435332]\n",
            "  ...\n",
            "  [-0.187291071 -0.925023 0.121136025 ... -0.512395501 -0.441329658 0.354023367]\n",
            "  [-0.527089834 -1.22049916 0.0345268063 ... 0.170702472 0.23843044 0.157179028]\n",
            "  [-0.341838181 -1.5209837 0.0613879375 ... -1.25804174 -0.587423205 0.211372718]]\n",
            "\n",
            " [[0.224181622 -0.139916822 -0.268119484 ... 0.0504326038 -0.0106474757 0.0067406483]\n",
            "  [-0.283415556 0.778559208 -0.348138 ... 0.0846240073 0.451650858 0.0725840628]\n",
            "  [0.0832337588 0.246459439 0.274271458 ... -0.255712599 0.470471442 -0.41930142]\n",
            "  ...\n",
            "  [-0.203127399 -0.966006577 0.116787255 ... -0.538770258 -0.467644274 0.357067645]\n",
            "  [-0.519761086 -1.21602476 0.0445338376 ... 0.181260273 -0.566842377 0.53357321]\n",
            "  [-0.240919635 -1.14638555 0.0611011349 ... -0.949317 -0.417510629 0.776028454]]\n",
            "\n",
            " [[0.231849149 0.00057250075 -0.263852656 ... 0.0569657348 -0.00498386472 -0.0325146168]\n",
            "  [-0.0290695745 0.328651488 -0.630732179 ... 0.534586966 -0.535944223 0.469367534]\n",
            "  [0.207173839 -0.756377459 -0.0771488 ... -0.835405529 0.101100013 0.0241140984]\n",
            "  ...\n",
            "  [-0.204279661 -0.952135086 0.108590499 ... -0.533676386 -0.462742567 0.344500422]\n",
            "  [-0.525674105 -1.22433341 0.0405567624 ... -0.603183568 -0.573197484 0.531190336]\n",
            "  [-0.240891725 -1.14325428 0.166000485 ... 0.192092419 -0.416725904 0.772511661]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.227729306 -0.136199147 -0.264435142 ... 0.0540692098 -0.00692777336 0.0103224926]\n",
            "  [-0.872663379 -1.26360822 0.612071574 ... 1.73787427 -1.03731072 0.181383386]\n",
            "  [0.679727793 -1.13246465 -0.496736974 ... 0.877706289 0.812583923 1.03135037]\n",
            "  ...\n",
            "  [0.19797 -0.921775758 0.121513695 ... 0.187513724 -0.439230949 0.173700556]\n",
            "  [-0.698082566 -1.6522696 0.0864929706 ... -0.798591256 -0.781037807 0.748978794]\n",
            "  [-0.231772721 -1.13087857 0.067998752 ... -0.935359597 -0.406604499 0.778102219]]\n",
            "\n",
            " [[0.234565333 -0.135022864 -0.264543742 ... 0.0584895276 -0.0043933019 0.0141722187]\n",
            "  [-0.633607388 0.257227957 0.672464907 ... 0.272491962 0.253519475 -0.255428463]\n",
            "  [-0.32028991 0.0146661317 -0.540566683 ... 0.194137096 -0.151879862 0.0555787571]\n",
            "  ...\n",
            "  [0.201797768 -0.909702659 0.125655577 ... -0.501388907 -0.430294156 0.356338441]\n",
            "  [-0.507842362 0.21809946 0.056679178 ... -0.584934592 -0.55460757 0.545702517]\n",
            "  [-0.229910597 -1.11732566 0.169816896 ... -0.924613893 -0.401851296 0.181976229]]\n",
            "\n",
            " [[0.223932728 -0.141921684 -0.270477563 ... 0.0494239 -0.0122579895 0.00560031459]\n",
            "  [-0.276509166 0.77101773 -0.340840727 ... 0.0804882646 0.449219197 0.808546185]\n",
            "  [-0.590132892 -0.0903888196 -0.773763716 ... 1.09156215 0.588410795 -0.0442057662]\n",
            "  ...\n",
            "  [-0.174584225 -0.915712714 0.135648221 ... 0.202047899 0.270223171 0.369469196]\n",
            "  [0.172491536 -1.22939098 0.0255853496 ... -0.61309576 -0.583045125 0.512697875]\n",
            "  [-0.253350258 -1.18100893 0.16577518 ... -0.978586853 -0.435633272 0.788688362]]], [[[0.138037086 0.0257256478 -0.106139988 ... 0.0292949118 -0.0148063134 -0.0400309116]\n",
            "  [-1.37376094 0.390788704 -0.754093945 ... 0.0544345379 0.564794838 0.834893286]\n",
            "  [-0.760274351 0.655962348 0.359618038 ... -0.273104638 0.377155393 -1.21430683]\n",
            "  ...\n",
            "  [-0.46685493 -0.64097476 0.177008539 ... -0.807929873 0.124587767 0.245580241]\n",
            "  [-0.47521162 0.0217091739 0.00568919443 ... -0.577637911 0.393679947 0.0361902118]\n",
            "  [-0.282555431 -0.0801724121 0.00817819685 ... -0.866189718 0.112151407 0.126456097]]\n",
            "\n",
            " [[0.1328329 0.0405154228 -0.0705299 ... 0.0161551405 -0.0193738956 -0.0248522796]\n",
            "  [-1.31888723 0.149942219 -0.765378177 ... -0.00725588761 0.494930327 0.029487215]\n",
            "  [-0.282727182 0.706462741 -0.0336757153 ... -0.233165145 0.473970085 -0.96256]\n",
            "  ...\n",
            "  [-0.400329739 -0.109404214 0.458971828 ... -0.875095844 0.314839333 0.247351393]\n",
            "  [-0.445428699 -0.217572197 0.0648344755 ... -0.433145404 0.0486999 0.187349454]\n",
            "  [-0.170819417 0.0430214852 0.140110701 ... -1.13809943 0.0794299841 0.477353752]]\n",
            "\n",
            " [[0.144382909 0.0949656665 -0.0721783042 ... -0.00705368817 -0.00645039231 -0.0408325866]\n",
            "  [-0.761563599 0.832281 -0.545951068 ... 0.857378244 -0.110465124 0.882146358]\n",
            "  [-0.273414403 -0.764554858 -0.666798711 ... -1.47592962 0.0107812211 0.330897927]\n",
            "  ...\n",
            "  [-0.497856051 0.145454943 0.051139228 ... -0.990161538 0.0426352955 0.306433082]\n",
            "  [-0.370637149 -0.131495416 0.0677332133 ... -0.481626868 -0.00554782152 0.296324372]\n",
            "  [-0.3226735 -0.212641239 0.0691673234 ... -0.498178601 0.115332194 0.476109445]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.086098969 0.0292752534 -0.126774445 ... 0.00485334545 -0.0147205666 -0.0603675246]\n",
            "  [-1.07286084 -1.02512503 0.379107118 ... 1.34906065 -0.73140347 -0.192873612]\n",
            "  [0.341846377 -0.866989493 -0.680513322 ... 0.482922 0.706105947 1.24303854]\n",
            "  ...\n",
            "  [-0.13746506 0.264843434 -0.0343853459 ... 0.222093046 0.020383399 0.17704457]\n",
            "  [-0.437754899 -0.203647852 -0.0956941694 ... -1.10768759 0.0251521133 0.414552093]\n",
            "  [-0.282805979 0.0226264745 0.0581558421 ... -0.669229388 0.201191694 0.518766761]]\n",
            "\n",
            " [[0.169600129 0.000297576189 -0.0874069 ... 0.0201399066 -0.0182064194 -0.0237365682]\n",
            "  [-0.218724012 0.810385227 0.514605641 ... 0.305125475 0.293657839 -0.120422341]\n",
            "  [-0.693705618 0.390444845 -0.767782 ... 0.313385546 -0.386656 0.196671829]\n",
            "  ...\n",
            "  [-0.112102702 0.314677536 0.218098193 ... -1.13078165 0.232932061 0.12877363]\n",
            "  [-0.431806594 0.904397 0.066538848 ... -1.1320976 0.172366589 0.422289252]\n",
            "  [-0.452057511 0.0460054278 0.351211131 ... -1.41878641 0.382544488 0.0624293685]]\n",
            "\n",
            " [[0.169299215 -0.00375150144 -0.114784263 ... 0.00462716073 -0.0123215 -0.0089006573]\n",
            "  [-1.01120508 0.358901054 -0.659781933 ... -0.44837 0.605208158 0.999707162]\n",
            "  [-0.91754055 0.0640168637 -0.647153676 ... 0.674985945 0.785515368 0.0564948469]\n",
            "  ...\n",
            "  [-0.335824519 0.0816159695 0.0796185285 ... -0.535075963 0.585331082 0.0413080528]\n",
            "  [-0.19650574 -0.155290306 -0.03798116 ... -0.946992397 -0.0955330655 0.17804873]\n",
            "  [-0.353841573 -0.149549618 0.0375304446 ... -1.36015844 0.23934868 0.48661375]]], [[[0.379292428 0.00336058438 -0.0728286207 ... 0.145787269 0.216479704 -0.143972307]\n",
            "  [-0.898076594 -0.370773673 -0.624261498 ... 0.192762643 0.400993466 0.547601938]\n",
            "  [-0.8617419 -0.127655163 0.185918316 ... -0.0441209152 0.93385905 -0.679821]\n",
            "  ...\n",
            "  [-0.466328561 -0.353547543 -0.529815 ... -0.731807232 0.0844932795 0.0200586244]\n",
            "  [-0.351467609 -0.147328615 -0.636665642 ... -0.08394517 0.418648541 -0.210054874]\n",
            "  [-0.468724668 -0.0719687045 -0.450474679 ... -0.16876711 0.249206021 0.12597619]]\n",
            "\n",
            " [[0.447063923 -0.0165106058 -0.0460678935 ... 0.113176472 0.132747203 -0.0590788908]\n",
            "  [-0.876015067 -0.729646206 -0.374925762 ... 0.0310610868 0.593993 0.242239386]\n",
            "  [-0.753094077 0.197989374 -0.198350355 ... -0.348161399 0.359138101 -0.392729193]\n",
            "  ...\n",
            "  [-0.439880401 0.120607764 0.0309027284 ... -0.854409099 0.31006667 -0.406106949]\n",
            "  [-0.386862159 -0.129720926 0.12141335 ... -0.249007151 0.169603422 0.0133527]\n",
            "  [0.0380859189 0.104259104 -0.0687951818 ... -0.689550221 0.0911569148 0.0641146898]]\n",
            "\n",
            " [[0.467423856 0.0502389222 -0.039952971 ... 0.121143796 0.111599788 -0.0533313565]\n",
            "  [-0.464016706 0.647582412 -0.734177828 ... 0.4170008 0.0200874694 0.659144759]\n",
            "  [0.466127098 -1.39146376 -0.401345223 ... -1.05208671 0.118149936 0.164891392]\n",
            "  ...\n",
            "  [-0.186084509 -0.0134917349 -0.175634146 ... -0.447427064 0.0134515464 -0.00606299937]\n",
            "  [-0.192442536 -0.212952808 -0.258724064 ... -0.267798185 0.102441907 -0.0538085774]\n",
            "  [-0.304126441 0.00504498184 -0.409704447 ... -0.242874175 0.0720078498 -0.19835104]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.384945691 0.00111985207 -0.17234683 ... 0.131429136 -0.0257846434 -0.0423662215]\n",
            "  [-1.28028309 -1.32253194 0.157339126 ... 1.15601254 -0.549754322 -0.394993722]\n",
            "  [0.242113918 -1.19934154 -0.466497928 ... 0.374365598 0.575807452 1.51226044]\n",
            "  ...\n",
            "  [-0.106640302 0.393459082 -0.222699329 ... 0.359492034 0.190392941 -0.111536644]\n",
            "  [-0.293167144 0.186092436 -0.461810857 ... -0.82812047 0.0437841229 -0.302447051]\n",
            "  [-0.0641856566 0.114060104 -0.330668271 ... -0.615798771 0.129962087 0.0624021]]\n",
            "\n",
            " [[0.459530383 -0.0903969258 -0.150467813 ... 0.141203538 0.151973441 -0.0908085257]\n",
            "  [-0.301789641 0.0376378149 0.524387956 ... 0.770725 0.504612625 0.111284167]\n",
            "  [-0.237038061 0.208822012 -1.08873212 ... 0.479954153 0.348525465 0.374507874]\n",
            "  ...\n",
            "  [-0.12357159 0.587011 -0.123959526 ... -0.486496836 0.24995707 -0.129547432]\n",
            "  [-0.138871029 0.589232 -0.130187333 ... -0.724174201 0.23249431 -0.148652196]\n",
            "  [-0.12452694 0.174061298 -0.0693762 ... -0.816588879 0.361171156 -0.470871478]]\n",
            "\n",
            " [[0.489198327 0.0586430281 -0.107384101 ... 0.165605366 0.0777867436 -0.0670779]\n",
            "  [-0.54764086 0.0917276293 -0.621352732 ... -0.190883696 0.697532892 0.836743951]\n",
            "  [-1.27283144 -0.610101581 -0.227706924 ... 0.393327087 0.724787235 -0.218281776]\n",
            "  ...\n",
            "  [-0.231760755 0.0604439676 -0.00995081291 ... -0.705945253 0.823587 -0.22372]\n",
            "  [-0.123597018 -0.0700575709 0.00831848569 ... -0.909395874 -0.214070812 -0.218469054]\n",
            "  [-0.0871130079 0.111854628 -0.273048759 ... -1.22758734 0.0975228697 0.0642906129]]], [[[0.271499813 0.0182363763 -0.172898054 ... 0.0292683616 0.434719831 -0.216085911]\n",
            "  [-1.32459426 -0.509189665 -0.599372566 ... -0.397686273 0.818343282 1.48426425]\n",
            "  [-1.31220317 -0.958094239 0.0535992235 ... -0.0437634811 1.17056084 -0.229980201]\n",
            "  ...\n",
            "  [-0.0682944208 0.215992659 -0.542754233 ... -0.935738921 0.227043897 -0.230980039]\n",
            "  [-0.154282659 0.0642090365 -0.575897217 ... -0.0247918852 0.499389023 -0.104860306]\n",
            "  [-0.542393506 -0.187239096 -0.1661347 ... -0.0778344572 0.538495719 0.173037767]]\n",
            "\n",
            " [[0.387088388 0.211835086 -0.446838647 ... -0.00655859802 0.196323335 -0.170173407]\n",
            "  [-1.32518744 -0.857506514 -0.478776366 ... -0.291902721 0.914385557 1.07842469]\n",
            "  [-0.880598903 -0.368482232 -0.0209955443 ... 0.597415447 0.586351752 -0.446205735]\n",
            "  ...\n",
            "  [-0.321130335 0.457549274 -0.2801449 ... -1.0813266 0.355398387 -0.582053363]\n",
            "  [-0.410035163 -0.128200561 -0.0206446741 ... 0.024571918 0.137712806 0.00357537717]\n",
            "  [-0.214576364 0.188781917 -0.0287421104 ... -0.41895318 -0.0664357319 -0.302643299]]\n",
            "\n",
            " [[0.437159449 0.0465347841 -0.241654038 ... 0.0935244113 0.0360878557 -0.258467]\n",
            "  [-0.5952245 0.814270258 -0.644456506 ... 0.388663918 0.414906949 0.807511806]\n",
            "  [0.516151 -0.82488966 -0.323976934 ... -1.02131057 -0.00800680276 0.262842387]\n",
            "  ...\n",
            "  [-0.0365382 -0.0850627422 -0.121946223 ... -0.284807295 0.12222302 -0.0872168541]\n",
            "  [-0.450151145 -0.0129665732 -0.146242544 ... -0.0737312138 0.0844755396 -0.0637632161]\n",
            "  [-0.428611159 0.201633453 -0.324563086 ... -0.293687195 0.011034254 -0.370280325]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.458606154 0.182088047 -0.535217941 ... 0.108354971 0.137357786 -0.305026233]\n",
            "  [-0.879472911 -1.27174807 0.192876637 ... 0.816559255 -0.319441587 -0.202260137]\n",
            "  [-0.386154503 -1.034495 -0.223933369 ... 0.133796766 0.744605064 1.38226283]\n",
            "  ...\n",
            "  [-0.32863909 0.558897853 -0.100286722 ... 0.296384305 0.0294155832 -0.0318347886]\n",
            "  [-0.115624093 0.270394832 -0.527262032 ... -0.590889573 0.0217094179 -0.397166729]\n",
            "  [-0.0429943949 0.289704323 -0.40728724 ... -0.200589284 0.105616607 -0.235305011]]\n",
            "\n",
            " [[0.519655466 0.121446468 -0.585703492 ... 0.0968620777 0.219717145 -0.327162147]\n",
            "  [-0.127111673 -0.289371848 0.030189326 ... 0.764860034 1.02749705 0.912971556]\n",
            "  [-0.35005936 0.386129707 -1.03824222 ... -0.208094269 0.0712217316 0.5085302]\n",
            "  ...\n",
            "  [-0.0676864311 0.191822737 -0.2128153 ... -0.160220459 0.293028444 -0.188176274]\n",
            "  [-0.356836349 0.39407289 -0.036114417 ... -0.547662556 0.403904319 -0.0756227225]\n",
            "  [-0.318160653 0.299755663 0.158199906 ... -0.502171874 0.289467514 -0.266185254]]\n",
            "\n",
            " [[0.445444912 0.129473776 -0.402850121 ... 0.14932552 0.0838153511 -0.146104276]\n",
            "  [-1.00242937 0.146594256 -0.735855758 ... -0.543462336 1.29083991 1.49981546]\n",
            "  [-1.22189355 -1.01895058 -0.159934431 ... 0.876966715 0.999449134 0.0789510682]\n",
            "  ...\n",
            "  [-0.213214397 0.150284767 -0.147888273 ... -0.369770229 0.48600322 -0.206412762]\n",
            "  [-0.551166296 0.387019843 -0.009427635 ... -1.04191554 -0.630950093 -0.0136311799]\n",
            "  [-0.439005852 0.110105 -0.0762668326 ... -0.525569439 -0.0147774108 -0.217334479]]], [[[0.193372399 0.473385036 -0.142810673 ... -0.0054684151 0.293188632 -0.196696132]\n",
            "  [-1.73182416 -0.508128762 -0.684729576 ... -0.142697528 0.97581625 0.937097669]\n",
            "  [-1.31193674 -1.17996621 -0.493767977 ... 0.0224564485 0.884323597 -1.01913357]\n",
            "  ...\n",
            "  [-0.148130491 -0.00406951457 -0.229536742 ... -0.345323116 -0.201499611 -0.175763145]\n",
            "  [-0.313020319 0.0586822219 -0.411474407 ... 0.313815594 0.511476338 -0.176553041]\n",
            "  [-0.76505518 -0.430005372 0.16056183 ... 0.164057955 0.587205768 0.258366317]]\n",
            "\n",
            " [[0.254922658 0.633871 -0.364719361 ... 0.0845243782 0.0799757391 0.00549277663]\n",
            "  [-1.86055803 -0.948957562 -0.300331563 ... -0.139011517 0.90741837 1.07559502]\n",
            "  [-0.989676714 0.0169645213 -0.361269325 ... 0.860180378 0.562967837 -0.794111431]\n",
            "  ...\n",
            "  [-0.122019544 0.269009322 -0.247178435 ... -0.74427563 -0.0536307096 -0.669417322]\n",
            "  [-0.651512921 -0.0998642147 0.0433278307 ... 0.0248517282 0.227310702 0.0747277513]\n",
            "  [-0.470553726 0.133997 0.06659659 ... -0.454785109 0.223005161 -0.677737236]]\n",
            "\n",
            " [[0.312401026 0.400170624 -0.298258662 ... 0.213103101 0.0749628246 -0.0208091699]\n",
            "  [-0.44466126 0.855708957 -0.640934 ... 0.0124081373 0.479012519 0.770901144]\n",
            "  [0.350911856 -0.736253381 -0.13107124 ... -1.00856507 -0.108062506 0.203495324]\n",
            "  ...\n",
            "  [-0.148454264 -0.156595454 0.0210212171 ... 0.0825446695 0.208405048 -0.329866052]\n",
            "  [-0.253398776 -0.0944557935 -0.166037709 ... 0.0386430547 0.0894750878 -0.294126153]\n",
            "  [-0.263521552 0.373347074 -0.265395343 ... -0.462469161 0.0234965365 -0.349461704]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.248230368 0.427903384 -0.43111074 ... 0.203744113 0.254785985 -0.303438783]\n",
            "  [-0.500186563 -1.53143787 0.307033658 ... 0.80712992 -0.445479 0.0518297404]\n",
            "  [-0.848155558 -1.90711761 -0.282427967 ... 0.423445463 1.04260576 0.936193347]\n",
            "  ...\n",
            "  [-0.30210197 0.673466623 0.100091785 ... 0.615846157 -0.184679613 0.0373416543]\n",
            "  [-0.381768405 0.233013362 -0.3399266 ... -0.246811867 0.189582452 -0.649396241]\n",
            "  [-0.295250684 0.338384181 -0.198857039 ... 0.0382881202 0.224896818 -0.515977919]]\n",
            "\n",
            " [[0.234671503 0.638380527 -0.474116862 ... 0.0388113149 0.114077292 -0.120005503]\n",
            "  [-0.312292606 -0.604029477 -0.0349231809 ... 0.355108291 0.676181197 0.715056419]\n",
            "  [-0.158622384 0.349949598 -1.03996849 ... 0.130451992 -0.0821313 0.288817286]\n",
            "  ...\n",
            "  [-0.270552605 0.0963618457 0.125680298 ... -0.101089433 0.382912904 -0.271756768]\n",
            "  [-0.353774697 0.134561613 0.398630947 ... -0.485910714 0.399376214 -0.295935392]\n",
            "  [-0.412128985 0.0585243367 0.517396212 ... -0.515131235 0.217495024 -0.311785042]]\n",
            "\n",
            " [[0.167265579 0.417106271 -0.337740719 ... 0.133266181 -0.0885353908 0.077487804]\n",
            "  [-1.43548942 -0.232082888 -0.605646312 ... -0.0689826086 1.19052708 2.18508482]\n",
            "  [-1.35034478 -1.2816658 0.0441853181 ... 1.0947243 1.10141182 0.151177287]\n",
            "  ...\n",
            "  [-0.0236327779 0.143986076 0.15838252 ... 0.0902818292 0.0422360376 -0.138490379]\n",
            "  [-0.386092216 0.217918903 0.171540767 ... -0.734766 -0.947831511 0.384406358]\n",
            "  [-0.513728559 0.0802023262 0.229489952 ... -0.0766869262 -0.0949626043 -0.338250428]]], [[[0.136521071 0.437786132 -0.0791060552 ... -0.290577739 0.14832 -0.300151348]\n",
            "  [-1.48836231 -0.521214068 -0.676381886 ... -0.218738928 0.621414 0.942180574]\n",
            "  [-1.34665549 -1.34424651 0.0710567385 ... -0.151814 0.68839258 -0.221172988]\n",
            "  ...\n",
            "  [-0.435123801 0.00388367381 -0.242550105 ... -0.222742215 -0.55743736 -0.393799096]\n",
            "  [-0.499537587 0.0102058 -0.292683274 ... 0.231196404 0.301288038 -0.55401957]\n",
            "  [-1.0480572 -0.170664966 0.136484966 ... 0.079082787 0.460648358 -0.0852100253]]\n",
            "\n",
            " [[0.481521875 0.68657577 -0.249336183 ... -0.415573478 -0.346849531 0.00345041603]\n",
            "  [-1.71845329 -1.02458858 -0.425688058 ... -0.00865947828 0.395513445 1.22945845]\n",
            "  [-0.447040886 0.156542897 -0.216404706 ... 0.72721839 0.35989508 -0.384940326]\n",
            "  ...\n",
            "  [-0.363084704 -0.100136079 -0.210056871 ... -0.698202372 0.0294203721 -0.715926588]\n",
            "  [-0.481381238 0.0439557396 -0.0717164651 ... 0.275154829 0.111456253 0.198662132]\n",
            "  [-0.375473976 0.182041317 0.171910942 ... -0.180271864 -0.0680449 -0.686928749]]\n",
            "\n",
            " [[0.243032143 0.24852699 -0.186359316 ... 0.12029694 -0.142378375 -0.112854451]\n",
            "  [-0.909859 0.551547468 -0.281806737 ... 0.216742888 0.0574637055 0.568252146]\n",
            "  [0.337491691 -0.185926393 -0.30364266 ... -0.990858436 -0.182512194 0.260267735]\n",
            "  ...\n",
            "  [-0.017449934 -0.0261694379 -0.0750893429 ... 0.249490112 0.0718231723 -0.259474158]\n",
            "  [-0.0298286173 -0.152080104 -0.306979239 ... 0.39108029 0.163355425 -0.136167139]\n",
            "  [0.200379446 0.320158035 -0.15911895 ... -0.135345459 -0.0047092312 -0.134913683]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.324909687 0.68612659 -0.289378226 ... -0.111869588 0.0718075931 -0.435843825]\n",
            "  [-0.753148854 -1.61661017 -0.177951455 ... 1.36205184 -0.56791836 0.554653227]\n",
            "  [-0.757860243 -2.26009679 -0.202677429 ... 0.567554533 1.24459147 0.779693484]\n",
            "  ...\n",
            "  [-0.643304884 0.57062614 0.121037647 ... 1.08823693 -0.0915524885 0.329701394]\n",
            "  [-0.214664608 0.153518394 -0.217518985 ... 0.153121486 -0.323955715 -0.527728617]\n",
            "  [-0.491870165 0.181379452 -0.246570855 ... 0.407416642 -0.0787834749 -0.198427528]]\n",
            "\n",
            " [[0.207448632 0.628317356 -0.320709288 ... 0.158169076 -0.0344116315 -0.110289633]\n",
            "  [-0.468400419 -0.682919562 -0.298405588 ... 0.774636388 0.880672216 0.935774624]\n",
            "  [-0.0717775226 0.341590166 -0.612333 ... 0.461273789 0.0183259565 0.371499032]\n",
            "  ...\n",
            "  [-0.195615306 -0.334393919 -0.137003422 ... 0.0755796134 0.0876997262 -0.159377605]\n",
            "  [-0.324169546 0.0760419443 0.155019283 ... -0.438539326 0.311317861 -0.165924847]\n",
            "  [-0.40024814 -0.0547426045 0.453967124 ... -0.160601899 0.139706776 -0.251743764]]\n",
            "\n",
            " [[0.124745168 0.469256401 -0.23481077 ... -0.229107708 -0.226983786 -0.0192793719]\n",
            "  [-1.45925736 -0.624319375 -0.908475161 ... 0.304522246 1.14507616 2.11304283]\n",
            "  [-0.742466092 -1.12827682 -0.0610243157 ... 0.937083185 1.21460128 0.81035322]\n",
            "  ...\n",
            "  [-0.119497158 -0.0607988201 0.200905189 ... 0.553512096 -0.116747953 -0.0589866117]\n",
            "  [-0.308529139 0.240787834 0.122787252 ... -0.297709346 -0.767744958 -0.024522271]\n",
            "  [-0.312464237 0.218864262 -0.0698015466 ... 0.420655757 -0.304629922 -0.297090024]]], [[[0.279089302 0.393940151 0.201583445 ... -0.285561055 0.110681817 -0.242530227]\n",
            "  [-1.19886863 -0.466357291 -0.79346 ... -0.0387431383 0.600528955 0.901727319]\n",
            "  [-1.32837617 -1.1213671 -0.0764270648 ... 0.134434134 0.388447165 -0.190459341]\n",
            "  ...\n",
            "  [-0.333350271 0.0749076307 -0.0990153924 ... 0.0957459137 -0.447758317 -0.141852468]\n",
            "  [-0.323394656 0.152107835 -0.283388197 ... 0.393740922 0.218500957 -0.886761069]\n",
            "  [-1.06198967 -0.0549040921 -0.0397478864 ... 0.40371117 0.436871946 -0.308802843]]\n",
            "\n",
            " [[0.591878831 0.625506 0.0717335194 ... -0.229147956 -0.157138571 -0.0703676194]\n",
            "  [-1.49378383 -1.16964734 -0.400983065 ... 0.122642867 0.552433968 0.523348212]\n",
            "  [-0.171251819 0.279257268 -0.451812863 ... 0.72959739 0.320469856 -0.394942105]\n",
            "  ...\n",
            "  [-0.148488462 -0.116071127 -0.15345782 ... -0.557755768 0.0628046468 -0.465192229]\n",
            "  [-0.24079971 0.326973379 0.0202929694 ... 0.381202668 0.162368968 -0.0997757167]\n",
            "  [-0.151232451 0.136556849 0.167175561 ... -0.175684169 0.153682724 -1.03825521]]\n",
            "\n",
            " [[0.385411978 0.30560711 -0.0863854662 ... -0.0693637058 0.000721732154 -0.0329948254]\n",
            "  [-0.652489483 0.487045437 -0.341869295 ... 0.377238452 -0.225542128 0.138838693]\n",
            "  [0.354009449 -0.316808462 -0.212969065 ... -0.506564617 0.0466159582 0.194873691]\n",
            "  ...\n",
            "  [-0.110990584 0.117726646 -0.225105226 ... 0.181498632 0.0043742368 -0.322971046]\n",
            "  [-0.251349539 -0.157282174 -0.670609653 ... 0.662592471 0.08194419 -0.126774922]\n",
            "  [0.319786638 0.453190416 -0.18873927 ... 0.378777206 -0.0556847751 -0.176772296]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.551802397 0.482470542 -0.0752172843 ... 0.00479538925 0.256389499 -0.430921376]\n",
            "  [-0.170250058 -1.43433082 -0.703635216 ... 1.60750675 -0.309476227 0.241906404]\n",
            "  [-0.0900649503 -1.79656684 -0.284318775 ... 1.14493299 0.84430635 0.898025155]\n",
            "  ...\n",
            "  [-0.563884735 0.358565331 -0.048808448 ... 0.98365289 0.0678346157 0.6162467]\n",
            "  [-0.138835624 0.305774778 -0.127490282 ... 0.694283903 0.00564433169 -0.476115644]\n",
            "  [-0.329244345 0.0715025291 -0.232616723 ... 0.628133595 0.236122817 -0.559202075]]\n",
            "\n",
            " [[0.439909309 0.54635036 -0.110118888 ... 0.140094295 0.0740918294 -0.240435109]\n",
            "  [-0.192683399 -0.514082074 -0.472255945 ... 1.18446255 0.921732545 0.888119936]\n",
            "  [0.0418687053 0.702972472 -0.731584966 ... 0.613072097 -0.184768334 0.152899161]\n",
            "  ...\n",
            "  [0.23507084 0.438104838 -0.142083466 ... 0.460550606 0.238421023 -0.417925656]\n",
            "  [-0.0673382133 0.595604718 0.0786008239 ... -0.33198 0.583978295 -0.236394703]\n",
            "  [-0.114529826 0.427849263 0.507551074 ... 0.212805271 0.464361876 -0.301367134]]\n",
            "\n",
            " [[0.413878083 0.503349066 0.043107532 ... 0.0177147277 -0.0312269088 0.0916980356]\n",
            "  [-0.761693478 -0.124672703 -0.889109969 ... 0.736361623 1.15939319 1.45454907]\n",
            "  [-0.685353041 -0.827282965 -0.225825012 ... 0.853755176 1.13296 0.790949225]\n",
            "  ...\n",
            "  [-0.19132331 0.301591069 0.199485123 ... 0.420851976 -0.114923522 0.210563123]\n",
            "  [-0.479783088 0.339046478 0.0813673884 ... 0.0414706692 -0.821279228 0.0696259588]\n",
            "  [-0.386108488 0.322251052 0.0496366322 ... 0.580949664 -0.547557771 -0.44104743]]], [[[0.125049 0.493324071 -0.176834345 ... -0.741074 -0.160138234 -0.662237465]\n",
            "  [-0.589296401 -0.0466510318 -0.412469566 ... 0.178125277 0.385219574 0.784376085]\n",
            "  [-1.60248566 -0.500535846 0.708598316 ... -0.0164505541 0.304803401 -0.21313706]\n",
            "  ...\n",
            "  [-0.0556016341 -0.315378577 -0.230960578 ... -0.350081146 -0.405260205 -0.565507054]\n",
            "  [-0.198019862 0.168435529 -0.789938033 ... -0.0476955026 0.225128204 -1.05626106]\n",
            "  [-0.569794059 -0.202058762 -0.0845652595 ... -0.196928784 0.741142631 -0.722178161]]\n",
            "\n",
            " [[0.973760068 0.860065043 -0.206997275 ... -0.206493124 -0.0305885356 -0.125789791]\n",
            "  [-1.20718086 -0.845072865 -0.212440789 ... 0.0715127 0.56155616 0.607030094]\n",
            "  [-0.213760585 0.462840319 -0.0582353845 ... 0.280516773 0.0653513223 -0.418629229]\n",
            "  ...\n",
            "  [-0.188733041 -0.380733937 -0.054733254 ... -0.906763 -0.0639471784 -0.550535381]\n",
            "  [-0.460280389 0.379588127 -0.311867744 ... -0.105044723 0.377167046 -0.567176819]\n",
            "  [-0.106113024 0.471103072 -0.214456946 ... -0.729255676 0.170957923 -1.06533575]]\n",
            "\n",
            " [[0.33574754 0.343589723 -0.151434064 ... -0.587132633 0.260314822 -0.59614253]\n",
            "  [-0.75864017 0.434951931 -0.0890772 ... -0.0233665965 0.348937213 -0.508145332]\n",
            "  [0.305781603 -0.0724678636 0.554405808 ... -0.68145895 -0.00203476986 -0.177949682]\n",
            "  ...\n",
            "  [-0.0596796758 0.317922711 -0.518293321 ... 0.0175885446 0.092286922 -0.531146228]\n",
            "  [-0.086225614 0.220575154 -0.547117352 ... 0.293094814 -0.0526501909 0.0419128]\n",
            "  [0.486086726 0.380480915 -0.0207059681 ... 0.0902155489 -0.257397413 -0.44273141]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.973766685 0.410487056 -0.270185024 ... -0.451740801 0.583546221 -0.571070075]\n",
            "  [-0.239006639 -1.50253856 -0.636778593 ... 1.50516164 -0.653767586 -0.0187091082]\n",
            "  [0.105293542 -1.27280509 0.00367162377 ... 0.827012718 1.04787135 0.763571203]\n",
            "  ...\n",
            "  [-1.02763009 0.318785667 -0.165494859 ... 1.16082394 0.17288807 0.644218922]\n",
            "  [-0.170942143 0.372447699 -0.423542351 ... 0.699390113 -0.0616955571 -0.8132478]\n",
            "  [-0.33149913 0.325433105 -0.243358254 ... 0.206236795 0.175553516 -0.976695061]]\n",
            "\n",
            " [[0.0984270051 0.493968546 -0.341029584 ... 0.0832914859 -0.478146106 -0.447158158]\n",
            "  [-0.312791556 -0.679480612 -0.471140534 ... 1.01904047 0.721514225 0.668640196]\n",
            "  [-0.574538291 1.15639722 -0.649805129 ... 0.551804066 -0.251731306 -0.120620623]\n",
            "  ...\n",
            "  [-0.0624483824 0.551738262 0.191922769 ... -0.00900007226 0.194840044 -1.02407765]\n",
            "  [-0.22568953 0.497308642 -0.0450610444 ... -0.319192082 0.616109252 -0.822430432]\n",
            "  [-0.209212199 0.608881891 0.432386339 ... -0.157369971 0.296448171 -0.830321193]]\n",
            "\n",
            " [[0.72541374 0.433888644 -0.114198111 ... -0.57457155 -0.0355645604 -0.242175519]\n",
            "  [-0.409561366 0.20681721 -0.997965276 ... 0.223819613 1.03036 1.04052472]\n",
            "  [-0.352994412 -0.575682163 -0.10024441 ... 1.00439274 0.769694209 0.477796555]\n",
            "  ...\n",
            "  [-0.280945867 0.44704926 -0.21122846 ... 0.471178561 0.0922324657 0.182042539]\n",
            "  [-0.649621904 0.806558967 0.0145079717 ... 0.185481831 -0.774949431 -0.23406212]\n",
            "  [-0.199707255 0.354992896 0.0482057109 ... 0.347523928 -0.319891 -0.784782052]]], [[[-0.100218654 0.525143385 -0.122904122 ... -0.521632731 -0.199034736 -0.952411354]\n",
            "  [-0.617449105 -0.21992062 -0.325514108 ... 0.157622859 0.436585456 1.10527384]\n",
            "  [-1.30024695 -0.402102917 0.496404231 ... -0.350503117 -0.28279388 0.228014037]\n",
            "  ...\n",
            "  [0.0194030553 -0.267023355 -0.108269379 ... -0.210068703 -0.0875412598 -0.492978573]\n",
            "  [-0.216350362 0.0656334 -0.722013593 ... -0.102745146 0.546774566 -0.878563046]\n",
            "  [-0.413899571 -0.102558687 0.0762215406 ... 0.209261566 0.898171604 -0.58396852]]\n",
            "\n",
            " [[0.466053724 1.08730793 -0.100419812 ... -0.229107052 0.121050648 -0.411877513]\n",
            "  [-1.13137078 -0.8276577 -0.195864141 ... -0.00191564672 0.252699971 0.447670877]\n",
            "  [-0.248042449 0.641801 -0.00149137061 ... 0.0867068172 -0.242327228 -0.242991656]\n",
            "  ...\n",
            "  [-0.0557900853 -0.173547149 -0.322026223 ... -0.741156816 -0.328186095 0.0782112628]\n",
            "  [-0.464714289 0.634225 -0.222769558 ... 0.132163778 0.450870126 -0.699338]\n",
            "  [-0.121101238 0.765553117 -0.0515586361 ... -0.708394945 0.579584956 -0.74343133]]\n",
            "\n",
            " [[0.389434844 0.558723867 -0.189842045 ... -0.620040715 0.329504639 -0.737632453]\n",
            "  [-1.04294729 -0.00750445947 -0.322969735 ... -0.199118197 0.687145054 -0.314075053]\n",
            "  [0.332323641 -0.274611384 0.512899935 ... -0.508550763 0.0666409805 -0.409900546]\n",
            "  ...\n",
            "  [-0.00606999546 0.183073938 -0.549110293 ... 0.284691274 0.496156186 -0.235500455]\n",
            "  [-0.0705053657 0.11317414 -0.639736354 ... 0.156348407 0.417871922 0.0968065113]\n",
            "  [0.587852895 0.188078701 -0.0146542173 ... 0.331619233 -0.0664837956 -0.499563336]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.609860659 0.455123693 -0.224328235 ... -0.386649877 0.31688872 -0.827332675]\n",
            "  [-0.595119 -1.39680898 -0.306806058 ... 1.38148153 -0.563187659 0.326809943]\n",
            "  [-0.457021564 -0.884637892 0.337374151 ... 0.541186512 0.841121137 0.588009894]\n",
            "  ...\n",
            "  [-0.33769086 0.205663279 -0.198750779 ... 1.49209142 -0.110439323 1.13725603]\n",
            "  [-0.255736381 0.457664073 -0.0819100291 ... 0.854113817 0.412103921 -0.732912183]\n",
            "  [-0.324998409 0.402689785 -0.0266733635 ... 0.604605854 0.313181907 -0.880488694]]\n",
            "\n",
            " [[-0.329578966 0.490791231 -0.195129186 ... -0.116505429 -0.394980937 -0.611150682]\n",
            "  [-0.570605457 -0.86146158 0.168101668 ... 0.74184835 0.583110511 0.383054256]\n",
            "  [-0.392698556 0.604281545 -0.125658482 ... -0.0788436085 -0.141747043 0.299173713]\n",
            "  ...\n",
            "  [-0.0271640699 0.475439548 0.358266383 ... -0.189372703 0.897300482 -1.20617139]\n",
            "  [-0.0943925083 0.21315679 0.0584477112 ... -0.363030374 1.05709195 -0.869243562]\n",
            "  [-0.0240056179 0.550196111 0.730670452 ... -0.360988945 1.11284471 -0.857507169]]\n",
            "\n",
            " [[0.586913 0.785803199 -0.0597739741 ... -0.289483964 0.0543169938 -0.161704779]\n",
            "  [-0.375378549 0.077090323 -0.938368082 ... 0.0304833092 1.01022828 1.14050901]\n",
            "  [-0.313838512 -0.381649315 -0.596598089 ... 0.852520645 0.986721 0.321865261]\n",
            "  ...\n",
            "  [-0.457214117 0.14051488 -0.0863856077 ... 0.452699184 -0.318177223 0.00312483311]\n",
            "  [-0.519977689 0.762074947 0.0469004661 ... 0.00840647146 -0.810119152 0.127979949]\n",
            "  [-0.41889441 0.388274 -0.143409207 ... 0.29715094 -0.139470786 -0.691571772]]], [[[-0.00260187685 0.523694038 -0.514533758 ... -0.423295408 0.0161784068 -1.12805212]\n",
            "  [-0.867837191 0.0164057463 -0.309336394 ... 0.285364211 0.0803868771 0.875450969]\n",
            "  [-1.02555645 -0.327203155 0.673846483 ... -0.0748082101 0.132126689 0.44727242]\n",
            "  ...\n",
            "  [-0.0382179916 -0.0342672877 -0.778469 ... -0.540271223 -0.24236989 -0.390133768]\n",
            "  [-0.446541429 0.091075629 -0.916888654 ... -0.109936 0.208364964 -1.37497973]\n",
            "  [-0.410456 0.0445413217 -0.176984519 ... 0.109738052 0.593840122 -1.09055459]]\n",
            "\n",
            " [[0.155263752 0.918051839 -0.529284298 ... -0.308933169 0.285680056 -0.496108234]\n",
            "  [-1.15361738 -0.514503837 0.101434678 ... 0.24937205 -0.0709431 0.763808608]\n",
            "  [-0.185544804 0.889274836 0.254025578 ... 0.116933346 -0.073848173 -0.509449899]\n",
            "  ...\n",
            "  [0.0501743257 0.0866684392 -0.233744219 ... -0.128920346 -0.108909979 -0.702932715]\n",
            "  [-0.808742642 0.707362056 -0.311658561 ... 0.212334558 0.400883108 -0.743463516]\n",
            "  [-0.051369682 0.743928194 0.195883691 ... -0.570611715 0.722525299 -0.770987153]]\n",
            "\n",
            " [[0.344250202 0.534847 -0.436714858 ... -0.415860683 0.568518698 -0.693724036]\n",
            "  [-0.550023139 -0.16485104 0.0603901371 ... 0.307155818 0.808321834 -0.676380217]\n",
            "  [0.637615502 -0.0600865074 0.780914605 ... -0.164161816 0.325350732 -0.678192139]\n",
            "  ...\n",
            "  [0.121336505 0.171052665 -0.433840811 ... 0.444472849 0.580982625 -0.114137501]\n",
            "  [0.111366063 0.364167273 -0.558552444 ... 0.362968683 0.527655602 0.164886251]\n",
            "  [0.337832093 0.07768 -0.153813779 ... 0.541089594 -0.0964722335 -0.457040846]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.0686805844 0.330049962 -0.292489707 ... -0.665696263 0.350221574 -0.7090258]\n",
            "  [-0.41768986 -1.19092333 -0.0375819653 ... 1.42328346 -0.204770774 0.133126572]\n",
            "  [-0.901637852 -0.780236423 0.251532137 ... 0.596667647 0.792203426 0.358191]\n",
            "  ...\n",
            "  [-0.162434727 0.495827675 -0.156515747 ... 1.20054793 0.168294787 0.741237104]\n",
            "  [-0.113665953 0.630184174 -0.222550869 ... 0.627749443 0.521891415 -0.680611491]\n",
            "  [-0.545223653 0.353694409 -0.734223843 ... 0.908124387 0.422360688 -0.91226989]]\n",
            "\n",
            " [[-0.584785283 0.194012254 -0.622646 ... -0.228503957 0.0791276395 -0.308481306]\n",
            "  [-1.24284124 -0.79083544 0.42542702 ... 0.543255806 0.551613867 0.567596614]\n",
            "  [-1.0289284 0.173184991 -0.330132902 ... 0.126684576 -0.138674468 0.175505087]\n",
            "  ...\n",
            "  [-0.140806809 0.60269171 0.163436174 ... -0.327452809 0.613404274 -0.700288236]\n",
            "  [0.112813383 0.187161118 -0.157802865 ... -0.45369339 0.837602615 -0.253445834]\n",
            "  [0.160837919 0.597276151 0.572475493 ... -0.510483801 0.785006762 -0.63215363]]\n",
            "\n",
            " [[0.105034515 0.584364533 -0.688573182 ... -0.253749579 0.240234464 -0.263755918]\n",
            "  [-0.505715191 -0.0836992 -0.50033313 ... 0.170241818 0.652813196 0.971174836]\n",
            "  [-0.480292559 -0.150775194 -0.0289657246 ... 1.01192594 0.682801723 0.059479937]\n",
            "  ...\n",
            "  [-0.364697456 0.247608185 0.0784126818 ... 0.700272739 -0.47884956 0.313430667]\n",
            "  [-0.625814915 1.20771801 0.535744607 ... 0.0104942117 -1.23221898 0.224177793]\n",
            "  [0.0104619935 0.0562612563 -0.608266771 ... 0.163539872 -0.251263201 -0.579718232]]], [[[0.0524586514 0.739087045 -0.326764762 ... -0.245248944 0.537385404 -1.05080533]\n",
            "  [-0.617965281 -0.193616867 -0.466293633 ... 0.0754795671 0.690741122 0.33623451]\n",
            "  [-0.57968086 -0.433333278 0.716324508 ... -0.175439104 0.173906475 0.371179223]\n",
            "  ...\n",
            "  [0.0930962935 0.0585896 -0.755120277 ... -0.681308 0.32919842 -0.381695151]\n",
            "  [-0.269871891 0.282564044 -0.96262908 ... -0.00407323241 0.270455241 -1.23083472]\n",
            "  [-0.394952208 0.186539233 -0.36326015 ... 0.154505908 0.751995742 -1.05742884]]\n",
            "\n",
            " [[0.28528294 0.747664869 -0.760867119 ... -0.127989545 0.175718725 -0.617453575]\n",
            "  [-1.34223747 -0.258737594 -0.345612824 ... 0.31488952 0.333923042 0.719657481]\n",
            "  [-0.227915794 0.458589435 -0.0476379208 ... 0.114886031 0.156343758 -0.830212653]\n",
            "  ...\n",
            "  [-0.029848747 0.0351754166 -0.518246651 ... -0.109605 0.603725255 0.0840829313]\n",
            "  [-1.03851247 0.892033696 -0.634783149 ... 0.249930695 0.478545845 -0.587316096]\n",
            "  [-0.480122179 0.590236902 -0.337551922 ... -0.573031604 0.660196 -0.460700333]]\n",
            "\n",
            " [[0.143687934 0.828463912 -0.404254556 ... -0.156303898 0.84975332 -0.599516153]\n",
            "  [-0.730933547 -0.636605799 0.160676941 ... 0.180873454 1.19140804 -0.348630071]\n",
            "  [0.582725465 0.423350185 0.404541284 ... 0.557735145 0.729457438 -0.815660715]\n",
            "  ...\n",
            "  [-0.00806747377 0.443973511 -0.615300894 ... 0.798371851 0.422983766 -0.0142174363]\n",
            "  [0.0694938824 0.33275336 -0.621823847 ... 0.719412625 0.353174984 0.111247942]\n",
            "  [0.452784657 0.449267596 -0.311307162 ... 0.463981539 0.456459 -0.855972409]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.113903008 0.807186604 -1.0217694 ... -0.107336879 0.570326 -0.52674067]\n",
            "  [-0.792440891 -0.567679 -0.571221352 ... 1.07759714 0.790801 0.472313106]\n",
            "  [-0.669340134 -0.455198228 0.251329392 ... 0.714125693 0.744667232 0.261031806]\n",
            "  ...\n",
            "  [-0.477386087 0.614490032 -0.241720855 ... 1.59830725 -0.187814802 0.825313091]\n",
            "  [-0.540440917 0.648328245 -0.235221118 ... 0.672079146 0.50064075 -0.585279524]\n",
            "  [-0.63686806 0.635488689 -0.960060596 ... 0.601941168 0.49996078 -1.08150601]]\n",
            "\n",
            " [[-0.358447105 0.73133707 -0.851487219 ... 0.10652715 0.426294804 -0.564278781]\n",
            "  [-0.907511532 -0.312633544 0.0455891714 ... 0.43859157 0.93199271 0.425490379]\n",
            "  [-0.573449671 0.39852196 -0.370083332 ... 0.138368577 -0.186828241 0.00524653494]\n",
            "  ...\n",
            "  [-0.254906893 0.640746951 0.18235603 ... -0.0532825142 0.48932451 -0.78748]\n",
            "  [0.155148655 0.231628835 -0.399145663 ... -0.211420834 0.591197371 -0.302059233]\n",
            "  [0.201005816 0.509646595 0.165597171 ... -0.347842485 0.611185908 -0.790876925]]\n",
            "\n",
            " [[0.0772501156 0.356156021 -1.19721484 ... 0.0604320578 0.635232627 -0.547571242]\n",
            "  [-0.75608 -0.167581886 -0.688347042 ... 0.465936214 0.974821091 0.810596824]\n",
            "  [-0.544535697 -0.269603 -0.322619349 ... 0.945865 0.566211402 0.0390422046]\n",
            "  ...\n",
            "  [-0.476585209 0.00338386 0.0238977447 ... 1.19926274 -0.217723817 0.0263359547]\n",
            "  [-0.860203505 1.8650471 0.520747542 ... 0.0694330931 -1.24476445 0.300118983]\n",
            "  [-0.317100257 0.226801783 -0.818399847 ... 0.445005774 -0.0411797166 -0.500368118]]], [[[0.332641 0.4062213 -0.142976016 ... -0.175809965 0.35536173 -0.77660805]\n",
            "  [-0.631983697 -0.237227798 -0.28302747 ... 0.421302676 0.805117369 0.0546351597]\n",
            "  [-0.467998743 0.150231212 0.818123102 ... 0.311053723 0.208824396 0.178405792]\n",
            "  ...\n",
            "  [0.242572919 -0.0320641659 -0.653024495 ... -0.491273671 0.361060292 -0.771834314]\n",
            "  [-0.282562047 0.196740046 -0.624792278 ... -0.0308897775 0.0940414369 -1.59061015]\n",
            "  [-0.475618422 -0.238931566 -0.331999183 ... 0.255957335 0.565162539 -1.48516703]]\n",
            "\n",
            " [[0.052598536 0.945228755 -0.558956563 ... -0.261987269 0.278393865 -0.748093665]\n",
            "  [-1.22435629 -0.213256627 -0.171253771 ... 0.486351341 0.435153693 0.391375899]\n",
            "  [-0.221706837 0.972900033 0.13060315 ... -0.0207010061 0.249571219 -0.605188]\n",
            "  ...\n",
            "  [-0.463752627 -0.0290590711 -0.570017517 ... -0.994108081 0.737469435 -0.838956177]\n",
            "  [-0.937405467 0.611845076 -0.584926069 ... 0.470963091 0.581003785 -0.575707316]\n",
            "  [-0.533803642 0.374347895 -0.40040797 ... -0.3896119 0.914803088 -0.657637835]]\n",
            "\n",
            " [[0.35077095 0.85397172 -0.184243187 ... -0.158120543 0.544218719 -0.516274154]\n",
            "  [-0.894001126 -0.756633759 0.341060936 ... 0.498171657 0.899720192 -0.35196498]\n",
            "  [0.494624913 0.541816831 0.558312595 ... 0.869340122 0.358513623 -0.705318272]\n",
            "  ...\n",
            "  [-0.260039479 0.507005036 -0.354924142 ... 1.22351122 0.322174042 0.274638176]\n",
            "  [-0.0819975883 0.407114893 -0.373383224 ... 1.20515108 0.245741352 0.198689878]\n",
            "  [0.0836606324 0.282716572 -0.375934422 ... 0.687789321 0.27423358 -1.03427303]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.0431805179 0.876968324 -0.597903788 ... -0.145303234 0.528364599 -0.725750864]\n",
            "  [-0.909851849 -0.486533612 -0.509940088 ... 0.874189377 0.526444 0.251086563]\n",
            "  [-0.550311148 -0.267078817 0.397996 ... 0.675341547 0.763431132 -0.130016893]\n",
            "  ...\n",
            "  [-0.908086121 0.26983431 0.134364679 ... 1.89196908 -0.193427399 -0.215305]\n",
            "  [-0.557309 0.399181813 0.0448477045 ... 0.710663497 0.309011519 -0.795721948]\n",
            "  [-0.563656092 0.586633682 -0.432935119 ... 0.685821831 0.63149 -1.52004182]]\n",
            "\n",
            " [[-0.0594584681 0.830078 -0.515195191 ... -0.229482591 0.302164644 -0.568758786]\n",
            "  [-0.84236896 -0.0141321681 0.134329438 ... 0.278493434 0.760392606 0.403662741]\n",
            "  [-0.323890418 0.910774112 -0.240300357 ... 0.0894384086 0.0435452051 -0.238965064]\n",
            "  ...\n",
            "  [0.152489409 0.0604268126 0.484923244 ... 0.1183566 0.797672451 -1.11787951]\n",
            "  [0.503674 -0.425213605 -0.115700305 ... 0.0631245226 0.629712701 -0.783695281]\n",
            "  [0.0606462359 -0.139926314 0.613696277 ... -0.131485373 0.567120433 -0.933407247]]\n",
            "\n",
            " [[0.167026982 0.593830466 -0.78079319 ... -0.282268047 0.284042507 -0.540376067]\n",
            "  [-0.932108223 -0.298541337 -0.282091022 ... 0.709861934 1.15297902 0.149623185]\n",
            "  [-0.276354045 -0.294937134 0.0750904083 ... 0.80084151 0.640659213 -0.25540033]\n",
            "  ...\n",
            "  [-0.375029504 0.366442025 -0.386676669 ... 1.44592774 0.222981438 0.171559066]\n",
            "  [-0.776708186 1.51025379 0.642224729 ... 0.492133766 -1.00879228 0.0220614746]\n",
            "  [-0.513224185 0.237864688 -0.915964246 ... 0.654152453 -0.399373144 -0.372834712]]], [[[0.423632056 0.119937025 -0.104284689 ... -0.0863815174 0.318411857 -0.344368875]\n",
            "  [-0.113208517 -0.0442570746 -0.164878577 ... 0.258218437 0.570232451 -0.218577147]\n",
            "  [-0.0291956142 0.0871441811 0.615847349 ... 0.0871140808 0.139793247 -0.0558816418]\n",
            "  ...\n",
            "  [0.182121485 -0.0240318682 -0.258289874 ... -0.155331582 0.234784856 -0.242467761]\n",
            "  [0.0245979782 0.0689494461 -0.223642021 ... 0.0357196666 0.168082029 -0.408702523]\n",
            "  [-0.000358795747 -0.124147542 -0.0851743668 ... 0.156282097 0.247766495 -0.325561583]]\n",
            "\n",
            " [[0.041354239 0.602810681 -0.290742695 ... 0.0163604952 0.259543419 -0.319617957]\n",
            "  [-0.476702899 0.0213437546 -0.328321904 ... 0.493911177 0.368517488 0.0150875207]\n",
            "  [-0.045859959 0.698605835 0.19644098 ... 0.159378335 0.00929412059 -0.500785649]\n",
            "  ...\n",
            "  [-0.150085047 0.172228873 -0.145624831 ... -0.299331605 0.451271713 -0.320792735]\n",
            "  [-0.298009515 0.228469372 -0.160617054 ... 0.347359031 0.260086089 -0.0832452327]\n",
            "  [-0.126125887 0.22630021 -0.130304188 ... 0.131433174 0.359516948 -0.198441014]]\n",
            "\n",
            " [[0.302306294 0.556890488 -0.158245042 ... -0.0191889144 0.353386164 -0.315334469]\n",
            "  [-0.112406701 -0.0178675205 -0.0529032946 ... 0.186943471 0.384382486 -0.134706587]\n",
            "  [0.685587227 0.365361512 0.125041544 ... 0.507295132 0.129853666 -0.401065975]\n",
            "  ...\n",
            "  [0.0741045102 0.131193 -0.169986844 ... 0.596335053 0.153003395 0.17271325]\n",
            "  [0.150873974 0.408909112 -0.0848258212 ... 0.716094494 0.212024659 0.135457829]\n",
            "  [0.0307632238 0.128833324 -0.188990742 ... 0.220125288 0.309348136 -0.459705472]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.0434672646 0.64525646 -0.317818493 ... -0.0479183495 0.233888015 -0.584658265]\n",
            "  [-0.229828805 0.0352571607 -0.113786981 ... 0.274051189 0.251397371 -0.24678421]\n",
            "  [-0.302907377 -0.0830011144 0.0712511763 ... 0.331669688 0.582770526 -0.176071614]\n",
            "  ...\n",
            "  [-0.3004466 0.142406315 0.0915308669 ... 0.817082763 0.020613974 0.0718502104]\n",
            "  [-0.278019786 0.236216247 0.103603177 ... 0.332453817 0.182451934 -0.220070794]\n",
            "  [-0.242759854 0.366847396 -0.084008269 ... 0.210794926 0.48381409 -0.561662734]]\n",
            "\n",
            " [[0.221804649 0.454867631 -0.319937557 ... -0.206465155 0.466034263 -0.252352089]\n",
            "  [-0.133469135 0.0189397503 -0.0384979695 ... 0.254655272 0.468468755 0.36931777]\n",
            "  [0.0377276763 0.549372613 -0.0386021398 ... -0.0579830408 0.160216093 -0.0989642814]\n",
            "  ...\n",
            "  [0.169902772 0.0121421237 0.245977372 ... -0.026660068 0.280652434 -0.256012559]\n",
            "  [0.367669433 -0.155180052 0.0927431285 ... -0.05443988 0.374161512 0.0271504316]\n",
            "  [0.332869738 -0.0460471511 0.309843451 ... -0.136574879 0.199345306 -0.160263851]]\n",
            "\n",
            " [[0.254505664 0.548948348 -0.638401091 ... -0.208320439 0.411297202 -0.496829599]\n",
            "  [-0.113655537 0.0802414194 -0.162197098 ... 0.245010853 0.623756707 -0.0985125899]\n",
            "  [0.193028852 0.197680041 -0.0762239397 ... 0.575285 0.411553293 -0.0746015161]\n",
            "  ...\n",
            "  [-0.0780324787 0.177801743 -0.17372942 ... 0.511593223 0.0974406525 0.133860603]\n",
            "  [-0.198675379 0.480289251 0.409535974 ... 0.358648688 -0.338008583 0.144055784]\n",
            "  [-0.170635894 0.256079853 -0.330508947 ... 0.295521349 -0.0990013629 -0.0937810913]]]]\n",
            "q_inputs_id shape:  TensorShape([16, 48])\n",
            "q_segment_ids :  TensorShape([16, 48])\n",
            "q_input_masks :  TensorShape([16, 48])\n",
            "q_bert_embedding at layer -2 Shape:  13\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-211f38de6b2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     filepath=checkpoint_path, verbose=1, save_weights_only=True, save_best_only=False, period=1)\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mmedical_qa_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mmedical_qa_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    789\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m           \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m           initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m     \u001b[0;31m# Case 3: Symbolic tensors or Numpy array-like.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1514\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1515\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m   def evaluate_generator(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1245\u001b[0m           \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m           \u001b[0mreset_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1247\u001b[0;31m           output_loss_metrics=self._output_loss_metrics)\n\u001b[0m\u001b[1;32m   1248\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbolic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, inputs, targets, sample_weights, reset_metrics, output_loss_metrics)\u001b[0m\n\u001b[1;32m    293\u001b[0m           \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m           \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m           output_loss_metrics=output_loss_metrics))\n\u001b[0m\u001b[1;32m    296\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36m_process_single_batch\u001b[0;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[1;32m    233\u001b[0m               \u001b[0moutput_loss_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_loss_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m               \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m               training=training))\n\u001b[0m\u001b[1;32m    236\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         raise ValueError('The model cannot be run '\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36m_model_loss\u001b[0;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[1;32m    150\u001b[0m           \u001b[0mcurrent_loss_reduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m           \u001b[0mloss_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReductionV2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNONE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m           \u001b[0mweighted_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m           \u001b[0mloss_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_loss_reduction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m     94\u001b[0m     with ops.name_scope(scope_name, format(self.__class__.__name__),\n\u001b[1;32m     95\u001b[0m                         (y_pred, y_true, sample_weight)):\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       return losses_utils.compute_weighted_loss(\n\u001b[1;32m     98\u001b[0m           losses, sample_weight, reduction=self.reduction)\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, y_true, y_pred)\u001b[0m\n\u001b[1;32m    156\u001b[0m       \u001b[0mLoss\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \"\"\"\n\u001b[0;32m--> 158\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/docproduct/loss.py\u001b[0m in \u001b[0;36mqa_pair_cross_entropy_loss\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mqa_pair_cross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mq_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     similarity_matrix = tf.matmul(\n\u001b[1;32m     20\u001b[0m         a=q_embedding, b=a_embedding, transpose_b=True)\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9NOhL1iIg44",
        "colab_type": "text"
      },
      "source": [
        "# Junk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bweYyZHXL7L0",
        "colab_type": "code",
        "outputId": "f71655cd-54c6-4143-f025-3fec5aef06c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from docproduct.train_data_to_embedding import train_data_to_embedding\n",
        "\n",
        "# tf.compat.v1.enable_eager_execution()\n",
        "\n",
        "train_data_to_embedding(model_path='/content/drive/My Drive/DocAssistTraining/bertffn_crossentropy/bertffn/',\n",
        "                            data_path='/content/data/',\n",
        "                            output_path='/content/output.prkt',\n",
        "                            pretrained_path='/content/BioBertFolder/biobert_v1.0_pubmed_pmc/')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Reading /content/data/small.csv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1it [01:53, 113.97s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14f5nM2sL7Dl",
        "colab_type": "code",
        "outputId": "a2f8440f-df85-42da-e763-9b7da06da788",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "df = pd.read_parquet('/content/output.prkt')\n",
        "df.head(20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>Q_FFNN_embeds</th>\n",
              "      <th>A_FFNN_embeds</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Like 5 yrs ago to now I've had some reoccurrin...</td>\n",
              "      <td>test to see if you are a carrier of herpes sim...</td>\n",
              "      <td>[1.0962960720062256, 0.35729750990867615, -0.5...</td>\n",
              "      <td>[2.1191976070404053, 0.20118729770183563, -0.1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>My 6 year old daughter complains about vaginal...</td>\n",
              "      <td>Make sure she is not. Wearing tight fitting un...</td>\n",
              "      <td>[0.8566192388534546, 0.3873278796672821, -0.41...</td>\n",
              "      <td>[2.0420145988464355, 0.045309700071811676, -0....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>My 6 year old daughter complains about vaginal...</td>\n",
              "      <td>Take her to a Gyn. Gynecologists are trained t...</td>\n",
              "      <td>[0.8566192388534546, 0.3873278796672821, -0.41...</td>\n",
              "      <td>[2.107142448425293, 0.18660292029380798, -0.00...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>My Friend is experiencing irritation and she s...</td>\n",
              "      <td>Followup with doc. Some women use yoghurt as a...</td>\n",
              "      <td>[0.9020317792892456, 0.5287120342254639, -0.52...</td>\n",
              "      <td>[1.675483226776123, 0.28767189383506775, -0.22...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>My Friend is experiencing irritation and she s...</td>\n",
              "      <td>Worrisome. Sounds like a self treatment for va...</td>\n",
              "      <td>[0.9020317792892456, 0.5287120342254639, -0.52...</td>\n",
              "      <td>[1.5095821619033813, 0.22489629685878754, -0.3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>My VG is itchy and feels irritated, no pain, n...</td>\n",
              "      <td>Many women don't understand that yeast often c...</td>\n",
              "      <td>[1.2272748947143555, 0.0677652359008789, -0.31...</td>\n",
              "      <td>[1.7241419553756714, 0.28745684027671814, -0.3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>My anus has been itchy and burning. Went away ...</td>\n",
              "      <td>Red excoriated skin is likely from a dermatiti...</td>\n",
              "      <td>[1.0458486080169678, 0.14292249083518982, -0.1...</td>\n",
              "      <td>[1.4487098455429077, -0.15864232182502747, -0....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>My anus itching and my pubic hair also my insi...</td>\n",
              "      <td>Infection. This sounds like 2 separate issues:...</td>\n",
              "      <td>[0.9763892292976379, 0.38595932722091675, -0.3...</td>\n",
              "      <td>[2.031980037689209, 0.2574853003025055, -0.202...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>My anus itching and my pubic hair also my insi...</td>\n",
              "      <td>Burning. itching and burning can be from a yea...</td>\n",
              "      <td>[0.9763892292976379, 0.38595932722091675, -0.3...</td>\n",
              "      <td>[2.5956954956054688, 0.32757827639579773, -0.3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>My friend had redness in her vagina and and it...</td>\n",
              "      <td>Vaginitis? vulvitis? See her doctor.</td>\n",
              "      <td>[0.9602717161178589, 0.24477048218250275, -0.1...</td>\n",
              "      <td>[2.3519704341888428, 0.11928334832191467, -0.1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>My inner vagina lip is burning/stinging. No bu...</td>\n",
              "      <td>Get tested . Lumps, bumps, blisters and other ...</td>\n",
              "      <td>[1.3054753541946411, 0.12823589146137238, -0.2...</td>\n",
              "      <td>[1.9768234491348267, 0.06348630785942078, -0.1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>My left vaginal lip is swollen. I don't have p...</td>\n",
              "      <td>Get an exam. It's so hard to tell without visu...</td>\n",
              "      <td>[1.06465482711792, 0.28710800409317017, -0.324...</td>\n",
              "      <td>[1.959474802017212, 0.31009358167648315, -0.41...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>My perineum is achy and burns when pee. Normal...</td>\n",
              "      <td>Probably. Friction is a likely cause. If it do...</td>\n",
              "      <td>[1.0745892524719238, 0.01589241623878479, -0.3...</td>\n",
              "      <td>[2.2741098403930664, 0.30488210916519165, -0.7...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>My right labia was just recently swollen today...</td>\n",
              "      <td>Need tests. Abnormal itching and swelling may ...</td>\n",
              "      <td>[1.092416763305664, 0.42494893074035645, -0.18...</td>\n",
              "      <td>[1.8382463455200195, 0.3743756115436554, -0.34...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>My right side of my vaginal area is swollen I ...</td>\n",
              "      <td>Swollen Bartholin Cy. You need to see your doc...</td>\n",
              "      <td>[0.9862925410270691, 0.5215415358543396, -0.39...</td>\n",
              "      <td>[1.7209455966949463, 0.3301275670528412, -0.35...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>My urethra opening is swollen and inflammed. S...</td>\n",
              "      <td>Examination. You will need to see an obgyn for...</td>\n",
              "      <td>[0.7104231715202332, 0.0797986090183258, -0.28...</td>\n",
              "      <td>[2.0379326343536377, -0.028327470645308495, -0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>My urethral opening is swollen with a red dot ...</td>\n",
              "      <td>Do you have painful urination ? Infection of u...</td>\n",
              "      <td>[1.0263031721115112, 0.0236496664583683, 0.134...</td>\n",
              "      <td>[1.4499692916870117, -0.031901273876428604, -0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>My vagina has been producing a lot of discharg...</td>\n",
              "      <td>By seeing GYN MD. You may have either or both ...</td>\n",
              "      <td>[0.8969526290893555, 0.27209824323654175, -0.5...</td>\n",
              "      <td>[1.818895697593689, -0.0339525081217289, -0.36...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>My vagina has been slightly irritated/burning ...</td>\n",
              "      <td>Best to see GYN. No diagnosis possible from th...</td>\n",
              "      <td>[0.9727095365524292, 0.3030593991279602, -0.60...</td>\n",
              "      <td>[1.810991644859314, -0.2012990266084671, -0.23...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>My vagina has been slightly irritated/burning ...</td>\n",
              "      <td>This sounds like an infection like a yeast inf...</td>\n",
              "      <td>[0.9727095365524292, 0.3030593991279602, -0.60...</td>\n",
              "      <td>[2.0035433769226074, 0.5467036962509155, -0.00...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             question  ...                                      A_FFNN_embeds\n",
              "0   Like 5 yrs ago to now I've had some reoccurrin...  ...  [2.1191976070404053, 0.20118729770183563, -0.1...\n",
              "1   My 6 year old daughter complains about vaginal...  ...  [2.0420145988464355, 0.045309700071811676, -0....\n",
              "2   My 6 year old daughter complains about vaginal...  ...  [2.107142448425293, 0.18660292029380798, -0.00...\n",
              "3   My Friend is experiencing irritation and she s...  ...  [1.675483226776123, 0.28767189383506775, -0.22...\n",
              "4   My Friend is experiencing irritation and she s...  ...  [1.5095821619033813, 0.22489629685878754, -0.3...\n",
              "5   My VG is itchy and feels irritated, no pain, n...  ...  [1.7241419553756714, 0.28745684027671814, -0.3...\n",
              "6   My anus has been itchy and burning. Went away ...  ...  [1.4487098455429077, -0.15864232182502747, -0....\n",
              "7   My anus itching and my pubic hair also my insi...  ...  [2.031980037689209, 0.2574853003025055, -0.202...\n",
              "8   My anus itching and my pubic hair also my insi...  ...  [2.5956954956054688, 0.32757827639579773, -0.3...\n",
              "9   My friend had redness in her vagina and and it...  ...  [2.3519704341888428, 0.11928334832191467, -0.1...\n",
              "10  My inner vagina lip is burning/stinging. No bu...  ...  [1.9768234491348267, 0.06348630785942078, -0.1...\n",
              "11  My left vaginal lip is swollen. I don't have p...  ...  [1.959474802017212, 0.31009358167648315, -0.41...\n",
              "12  My perineum is achy and burns when pee. Normal...  ...  [2.2741098403930664, 0.30488210916519165, -0.7...\n",
              "13  My right labia was just recently swollen today...  ...  [1.8382463455200195, 0.3743756115436554, -0.34...\n",
              "14  My right side of my vaginal area is swollen I ...  ...  [1.7209455966949463, 0.3301275670528412, -0.35...\n",
              "15  My urethra opening is swollen and inflammed. S...  ...  [2.0379326343536377, -0.028327470645308495, -0...\n",
              "16  My urethral opening is swollen with a red dot ...  ...  [1.4499692916870117, -0.031901273876428604, -0...\n",
              "17  My vagina has been producing a lot of discharg...  ...  [1.818895697593689, -0.0339525081217289, -0.36...\n",
              "18  My vagina has been slightly irritated/burning ...  ...  [1.810991644859314, -0.2012990266084671, -0.23...\n",
              "19  My vagina has been slightly irritated/burning ...  ...  [2.0035433769226074, 0.5467036962509155, -0.00...\n",
              "\n",
              "[20 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXTxVGx2pyxw",
        "colab_type": "code",
        "outputId": "0494fdb3-1a80-4dba-de9f-62e6adb7003e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.shape(df.Q_FFNN_embeds)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=24368, shape=(2,), dtype=int32, numpy=array([ 50, 768], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5oF56kRsfTr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_true = tf.eye(tf.shape(df.Q_FFNN_embeds)[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHpGiaBYsfah",
        "colab_type": "code",
        "outputId": "7668feba-ba83-439a-c587-61e5057262ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "y_true"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=24421, shape=(50, 50), dtype=float32, numpy=\n",
              "array([[1., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 1., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 1., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 1., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKzI1Y8cuePB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "q_embedding = df.Q_FFNN_embeds\n",
        "a_embedding = df.A_FFNN_embeds\n",
        "similarity_matrix = tf.matmul(\n",
        "        a=q_embedding, b=a_embedding, transpose_b=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-E3r7JPsfXo",
        "colab_type": "code",
        "outputId": "2ab36f13-884d-4071-de14-e174f7de0ae5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "similarity_matrix"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=24372, shape=(50, 50), dtype=float64, numpy=\n",
              "array([[667.7659385 , 693.15932498, 653.16278176, ..., 676.82797045,\n",
              "        663.0307033 , 711.21231976],\n",
              "       [656.57736511, 689.9612447 , 652.60367404, ..., 674.5995719 ,\n",
              "        658.23500307, 705.80929928],\n",
              "       [656.57736511, 689.9612447 , 652.60367404, ..., 674.5995719 ,\n",
              "        658.23500307, 705.80929928],\n",
              "       ...,\n",
              "       [690.9488539 , 722.29331749, 677.60918259, ..., 701.22186118,\n",
              "        687.42525986, 745.86759126],\n",
              "       [663.90166992, 691.34643131, 650.98480913, ..., 676.07821412,\n",
              "        659.47771479, 704.0814083 ],\n",
              "       [663.90172161, 691.34648363, 650.98485881, ..., 676.078267  ,\n",
              "        659.47776484, 704.08146074]])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pR8-II6dpy2Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "similarity_matrix_softmaxed = tf.nn.softmax(similarity_matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VYYHgQku4LZ",
        "colab_type": "code",
        "outputId": "47b10755-1950-4ef3-cd73-73884f03d78f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "similarity_matrix_softmaxed"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=24374, shape=(50, 50), dtype=float64, numpy=\n",
              "array([[1.84702354e-30, 1.97097018e-19, 8.40237459e-37, ...,\n",
              "        1.59243934e-26, 1.62175748e-32, 1.36457079e-11],\n",
              "       [4.73812145e-32, 1.49293493e-17, 8.90951539e-34, ...,\n",
              "        3.18090827e-24, 2.48604625e-31, 1.13962910e-10],\n",
              "       [4.73812145e-32, 1.49293493e-17, 8.90951539e-34, ...,\n",
              "        3.18090827e-24, 2.48604625e-31, 1.13962910e-10],\n",
              "       ...,\n",
              "       [5.68854713e-34, 2.33200211e-20, 9.15494464e-40, ...,\n",
              "        1.64631045e-29, 1.67773712e-35, 4.03557991e-10],\n",
              "       [2.30349482e-29, 1.91203425e-17, 5.65803819e-35, ...,\n",
              "        4.47294125e-24, 2.76113689e-31, 6.48971909e-12],\n",
              "       [2.30349233e-29, 1.91203341e-17, 5.65802074e-35, ...,\n",
              "        4.47294177e-24, 2.76112939e-31, 6.48971706e-12]])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "We8bsvN6Oulv",
        "colab_type": "code",
        "outputId": "9232e18d-72ef-4db8-bad1-0f7123cfd8df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "y_true = tf.argmax(y_true, axis=1)\n",
        "y_pred = tf.argmax(similarity_matrix, axis=1)\n",
        "print(y_true)\n",
        "print(y_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
            " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
            " 48 49], shape=(50,), dtype=int64)\n",
            "tf.Tensor(\n",
            "[18 45 45 45 45 18 18 18 18 45 45 45 18 45 45 18 18 45 18 18 18 45 45 45\n",
            " 45 45 45 45 45 18 18 45 45 45 45 45 18 18 18 18 18 18 18 18 18 18 18 18\n",
            " 45 45], shape=(50,), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHjQKAPrRi_M",
        "colab_type": "code",
        "outputId": "2d703596-7810-483b-fc1e-6f854ab4b4be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "tf.cast(tf.equal(y_pred, y_true), tf.float32)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=24438, shape=(50,), dtype=float32, numpy=\n",
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZGiI8qPPDRa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "acc = tf.reduce_mean(tf.cast(tf.equal(y_pred, y_true), tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4M9z0M_pPvHX",
        "colab_type": "code",
        "outputId": "8da2e2e5-6848-4656-bb70-5f72aa2b43ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "acc"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=24432, shape=(), dtype=float32, numpy=0.02>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GDpB4w9wxCl",
        "colab_type": "code",
        "outputId": "2b9c6b3b-2c16-4e32-fe67-9d108037dd2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.argmax(similarity_matrix_softmaxed[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=24381, shape=(), dtype=int64, numpy=18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoN5bzSFva9P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss = tf.keras.losses.categorical_crossentropy(y_true, similarity_matrix_softmaxed, from_logits=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_91Tw04viyy",
        "colab_type": "code",
        "outputId": "623f5341-3b9b-4596-8165-a0037fcf3240",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "loss"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=24719, shape=(50,), dtype=float64, numpy=\n",
              "array([16.11809565, 16.11809565, 16.11809565, 16.11809565, 16.11809565,\n",
              "       16.11809565, 16.11809565, 16.11809565, 16.11809565, 16.11809565,\n",
              "       16.11809565, 16.11809565, 16.11809565, 15.07356181, 16.11809565,\n",
              "       16.11809565, 16.11809565, 16.11809565,  2.95342164, 16.11809565,\n",
              "       16.11809565, 16.11809565, 16.11809565, 16.11809565, 16.11809565,\n",
              "       16.11809565, 16.11809565, 16.11809565, 16.11809565, 16.11809565,\n",
              "       16.11809565, 16.11809565, 16.11809565, 16.11809565, 16.11809565,\n",
              "       16.11809565, 16.11809565, 16.11809565, 16.11809565, 16.11809565,\n",
              "       16.11809565, 16.11809565, 16.11809565, 16.11809565, 16.11809565,\n",
              "        2.06206044, 16.11809565, 16.11809565, 16.11809565, 16.11809565])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pzdBSdPBnRL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels_cce  = K.variable([[1,    0,  0], [0,    1,  0], [0,   0,   1]])\n",
        "preds       = K.variable([[.90,.05,.05], [.50,.89,.60], [.05,.01,.94]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4rd9VVzA9VB",
        "colab_type": "code",
        "outputId": "e12ba532-1ded-401e-871f-b5e2f61eeab9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "losses = []\n",
        "\n",
        "np_labels = K.get_value(labels_cce)\n",
        "np_preds  = K.get_value(preds)\n",
        "\n",
        "for label, pred in zip(np_labels, np_preds):\n",
        "    pred /= pred.sum(axis=-1, keepdims=True)\n",
        "    print('pred : ',pred)\n",
        "    print('label : ',label)\n",
        "    losses.append(np.sum(label * -np.log(pred), axis=-1, keepdims=False))\n",
        "print(losses)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pred :  [0.9  0.05 0.05]\n",
            "label :  [1. 0. 0.]\n",
            "pred :  [0.2512563  0.44723618 0.30150756]\n",
            "label :  [0. 1. 0.]\n",
            "pred :  [0.05 0.01 0.94]\n",
            "label :  [0. 0. 1.]\n",
            "[0.105360545, 0.8046685, 0.061875407]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTjjuxnlA9db",
        "colab_type": "code",
        "outputId": "0a236959-b55e-48ed-cb3b-e2f2930d92ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pred = [3.49840608e-29 , 0.05, 0.05]\n",
        "-np.log(pred) * [1,0,0]\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([65.52266024,  0.        ,  0.        ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0n4SGxly6RFg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYLGBbGUA9aR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = [3.49840608e-29, 5.21724993e-17, 1.32909681e-37, 8.69946029e-24,\n",
        "       6.42280146e-19, 1.06560527e-22, 3.20415741e-29, 3.07536680e-32,\n",
        "       2.19788681e-13, 1.47057614e-25, 2.37063794e-23, 4.86786939e-22,\n",
        "       6.53078132e-32, 1.19627823e-04, 4.75464243e-22, 1.11019827e-20,\n",
        "       1.30980806e-31, 3.30851996e-24, 5.13415064e-01, 5.57493394e-28,\n",
        "       4.55147633e-12, 1.01944511e-24, 4.30757341e-12, 2.23792386e-20,\n",
        "       2.15366987e-19, 1.18201880e-19, 1.06912472e-19, 6.11925136e-19,\n",
        "       1.21602987e-24, 1.86776670e-58, 3.30901644e-14, 1.43814301e-25,\n",
        "       7.44197436e-22, 3.90050859e-26, 2.75638425e-28, 1.26396912e-49,\n",
        "       1.09015236e-12, 3.65601432e-19, 8.72899811e-16, 5.22431074e-37,\n",
        "       1.68621718e-41, 1.16407291e-23, 1.29391908e-13, 6.41339662e-31,\n",
        "       2.43685044e-29, 4.86465308e-01, 5.52473964e-29, 1.42386132e-26,\n",
        "       4.90662621e-31, 6.65340795e-12]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPJUSM_jFz3f",
        "colab_type": "code",
        "outputId": "67d42b17-00d6-4888-b36c-85bfc3153bfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.sum(x)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9999999998399859"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wE-tiGBFFbaR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " x /= sum(np.array(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHcbgF-CFnMy",
        "colab_type": "code",
        "outputId": "79119d91-a891-46b7-a499-8fcf5ad0530f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x.sum()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opn18uAoFOgI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
        "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
        "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3do4_PhON47",
        "colab_type": "code",
        "outputId": "298db327-c037-42b0-db09-431009fb572f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "-np.log(x) * y"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([65.52266024,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYhbIsLstfVc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "bb083035-ff8c-4a06-adde-ec68b6e660db"
      },
      "source": [
        "t = tf.constant([\n",
        "  [[0, 1, 2, 3, 4],\n",
        "   [5, 6, 7, 8, 9]],\n",
        "  [[10, 11, 12, 13, 14],\n",
        "   [15, 16, 17, 18, 19]],\n",
        "  [[20, 21, 22, 23, 24],\n",
        "   [25, 26, 27, 28, 29]]])\n",
        "print(t)\n",
        "print(t[-2])\n",
        "\n",
        "# tf.reshape(t, [-1])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[[ 0  1  2  3  4]\n",
            "  [ 5  6  7  8  9]]\n",
            "\n",
            " [[10 11 12 13 14]\n",
            "  [15 16 17 18 19]]\n",
            "\n",
            " [[20 21 22 23 24]\n",
            "  [25 26 27 28 29]]], shape=(3, 2, 5), dtype=int32)\n",
            "tf.Tensor(\n",
            "[[10 11 12 13 14]\n",
            " [15 16 17 18 19]], shape=(2, 5), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ckszcNAwJlG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5846a4d4-5c6d-4d70-d50f-c3c128ab10d9"
      },
      "source": [
        "len([25, 26, 27, 28, 29])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    }
  ]
}